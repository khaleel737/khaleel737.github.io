<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blogs on Max Halford</title><link>https://maxhalford.github.io/blog/</link><description>Recent content in Blogs on Max Halford</description><generator>Hugo</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Sat, 08 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Minimizing the runtime of a SQL DAG</title><link>https://maxhalford.github.io/blog/minimizing-sql-dag-runtime/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/minimizing-sql-dag-runtime/</guid><description>&lt;p>I recently looked into reducing the runtime of &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a>&amp;rsquo;s SQL DAG. Our DAG is made up of roughly 150 SQL queries. It takes roughly 10 minutes to run, using BigQuery &amp;ndash; with on-demand pricing. This is quite decent. However, the results of our DAG feed customer dashboards, and we have the (bad) habit of refreshing the DAG several times a day. Reducing the runtime by a few minutes can thus be a nice quality-of-life improvement.&lt;/p></description></item><item><title>Hard data integration problems at Carbonfact</title><link>https://maxhalford.github.io/blog/hard-data-integration-problems-at-carbonfact/</link><pubDate>Thu, 02 Jan 2025 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/hard-data-integration-problems-at-carbonfact/</guid><description>&lt;p>Carbonfact&amp;rsquo;s customers are clothing brands and factories. Our mission is to measure (and ultimately reduce) the carbon footprint of their products. We need primary data to do this: purchase orders, bills of materials, energy consumption data, etc.&lt;/p>
&lt;p>Each customer has a unique IT setup, which makes it challenging to scale to hundreds/thousands of customers. Our success as a business depends on our ability to not reinvent the wheel for each customer.&lt;/p></description></item><item><title>Introducing icanexplain @ PyData Paris 2024</title><link>https://maxhalford.github.io/blog/icanexplain-pydata/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/icanexplain-pydata/</guid><description/></item><item><title>@daily_cache implementation in Python</title><link>https://maxhalford.github.io/blog/python-daily-cache/</link><pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/python-daily-cache/</guid><description>&lt;p>I spend a lot of time at Carbonfact working on datasets shared by our customers. We typically set things up so that our customers can export data automatically. They usually deposit files to a GCP bucket, with a script, once a day. We then have an ETL script for each customer that runs afterwards to fetch their latest data and process it.&lt;/p>
&lt;p>During development, I load customer data to my laptop and work on it. The datasets can be quite heavy, and it takes time to fetch them, so I cache them to save some time. Python has &lt;a href="https://docs.python.org/3/library/functools.html">something&lt;/a> for this in its standard library:&lt;/p></description></item><item><title>LCA software: exit the matrix</title><link>https://maxhalford.github.io/blog/lca-exit-the-matrix/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/lca-exit-the-matrix/</guid><description>&lt;p>Measuring the environmental impact of a product is done using &lt;a href="https://en.wikipedia.org/wiki/Life-cycle_assessment">life cycle assessment&lt;/a> (LCA). This is a methodology that breaks down a product&amp;rsquo;s life cycle into stages (&lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment#Life_cycle_inventory_(LCI)">LCI&lt;/a>), and measures the impact of each stage on the environment (&lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment#Life_cycle_impact_assessment_(LCIA)">LCIA&lt;/a>).&lt;/p>
&lt;p>There are a few pieces of LCA software to choose from. The leading ones are &lt;a href="https://simapro.com/">SimaPro&lt;/a>, &lt;a href="https://sphera.com/life-cycle-assessment-lca-software/">GaBi&lt;/a>, &lt;a href="https://www.openlca.org/">openLCA&lt;/a>, and &lt;a href="https://www.ifu.com/umberto/">Umberto&lt;/a>. These are all proprietary software, and they&amp;rsquo;re expensive. But there&amp;rsquo;s a free and open source alternative: &lt;a href="https://docs.brightway.dev/en/latest/">Brightway&lt;/a>.&lt;/p></description></item><item><title>Cutting up shoes to measure their footprint</title><link>https://maxhalford.github.io/blog/cutting-up-shoes/</link><pubDate>Fri, 17 May 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/cutting-up-shoes/</guid><description>&lt;p>Our mission at &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a> is to measure the environmental impact of clothes. This involves a lot of steps. The main one is to determine what materials a product is made of, along with each material&amp;rsquo;s mass. This is straightforward for most clothes like jumpers and pants. These are typically made of a single fabric, such as cotton or polyester. The mass of each material is roughly the same as the product&amp;rsquo;s mass.&lt;/p></description></item><item><title>A training set for bike sharing forecasting</title><link>https://maxhalford.github.io/blog/bike-sharing-forecasting-training-set/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/bike-sharing-forecasting-training-set/</guid><description>&lt;style>
table {
 font-family: monospace; /* Apply monospace font */
}

table td, table th {
 white-space: nowrap; /* Prevent text from wrapping */
}
&lt;/style>
&lt;p>Last night I went to a &lt;a href="https://www.meetup.com/fr-FR/tlse-data-science/">Toulouse Data Science&lt;/a> meetup. The talks were about generative AI and information retrieval, which aren&amp;rsquo;t topics I&amp;rsquo;m knowledgeable about. However, one of the speakers was a &lt;a href="https://github.com/raphaelsty">friend&lt;/a> of mine, so I went to support him. Toulouse is my hometown, so I bumped into a few people I knew. It was a nice evening.&lt;/p></description></item><item><title>Fast Poetry and pre-commit with GitHub Actions</title><link>https://maxhalford.github.io/blog/fast-poetry-pre-commit-github-actions/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fast-poetry-pre-commit-github-actions/</guid><description>&lt;p>This is a short post to share a GitHub Actions pattern I use to setup &lt;a href="https://python-poetry.org/">Poetry&lt;/a> and &lt;a href="https://pre-commit.com/">pre-commit&lt;/a>. These two tools cover most of my Python development needs. I use Poetry to manage dependencies and pre-commit to run code checks and formatting. The setup is fast because it caches the virtual environment and the &lt;code>.local&lt;/code> directory.&lt;/p>
&lt;p>I like to use &lt;a href="https://docs.github.com/en/actions/creating-actions/about-custom-actions">custom actions&lt;/a> for this type of stuff. These are base actions that canÂ be re-used in multiple workflows. I have a custom action to install the Python environment. Here&amp;rsquo;s the action file:&lt;/p></description></item><item><title>Decomposing funnel metrics</title><link>https://maxhalford.github.io/blog/funnel-decomposition/</link><pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/funnel-decomposition/</guid><description>&lt;h2 id="funnel-metrics-as-products">Funnel metrics as products&lt;/h2>
&lt;p>I talked about metric decomposition in a &lt;a href="https://maxhalford.github.io/blog/kpi-evolution-decomposition">previous article&lt;/a>, and how it can be used to explain why metrics change values over time. That article explained how to decompose a sum, as well as a ratio. In this article, I&amp;rsquo;ll explain how to decompose a product.&lt;/p>
&lt;pre tabindex="0">&lt;code>revenue = impressions * click_rate * conversion_rate * spend
&lt;/code>&lt;/pre>&lt;p>The decomposition in this article isn&amp;rsquo;t limited to funnels. It can be applied to any metric that is expressed as a product of factors. For instance, at Carbonfact, we decompose the carbon footprint of a clothing line as so:&lt;/p></description></item><item><title>Efficient ELT refreshes</title><link>https://maxhalford.github.io/blog/efficient-data-transformation/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/efficient-data-transformation/</guid><description>&lt;p>A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the &lt;em>de facto&lt;/em> standard for modern data teams, and is epitomized by &lt;a href="https://www.getdbt.com/">dbt&lt;/a> and its ecosystem. It&amp;rsquo;s a great time to be a data engineer!&lt;/p>
&lt;p>We at &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a> fully embrace the ELT paradigm. In fact, our whole platform is powered by BigQuery, which acts as our single source of truth. We have a main BigQuery dataset where we materialize severalÂ SQL views that power what our customers see.&lt;/p></description></item><item><title>Online machine learning on the road @ IDE+A, TH KÃ¶ln</title><link>https://maxhalford.github.io/blog/online-machine-learning-on-the-road/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-on-the-road/</guid><description/></item><item><title>Sh*t flows downhill, but not at Carbonfact</title><link>https://maxhalford.github.io/blog/shit-flows-downhill-but-not-at-carbonfact/</link><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/shit-flows-downhill-but-not-at-carbonfact/</guid><description>&lt;p>I&amp;rsquo;m writing this after watching the talk &lt;a href="https://josephreis.com/">Joe Reis&lt;/a> gave at &lt;a href="https://bigdataldn.com/">Big Data LDN&lt;/a>. It&amp;rsquo;s called &lt;a href="https://www.youtube.com/watch?app=desktop&amp;amp;v=OCClTPOEe5s&amp;amp;ref=blef.fr">Data Modeling is Dead! Long Live Data Modeling!&lt;/a> It&amp;rsquo;s an easy-to-watch short talk that calls out on a few modern issues in the data world.&lt;/p>
&lt;p>I&amp;rsquo;d like to bounce off one of Joe&amp;rsquo;s slides:&lt;/p>
&lt;div align="center" >
&lt;figure style="width: 80%; margin: 0;">
 &lt;img src="https://maxhalford.github.io/img/blog/shit-flows-downhill-but-not-at-carbonfact/shit-flows-downhill.png">
&lt;/figure>
&lt;/div>
&lt;p>I&amp;rsquo;m aligned with Joe that many issues stem from the lack of unison between engineering and data teams. A fundamental aspect of the Modern Data Stack is to replicate/copy production data into an analytics warehouse. For instance, copying the production PostgreSQL database into BigQuery.&lt;/p></description></item><item><title>Answering "Why did the KPI change?" using decomposition</title><link>https://maxhalford.github.io/blog/kpi-evolution-decomposition/</link><pubDate>Wed, 09 Aug 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/kpi-evolution-decomposition/</guid><description>&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>I published a notebook &lt;a href="https://gist.github.com/MaxHalford/9fba0c2d6800d0f0643902bf57b99780">here&lt;/a> that deals with the case where dimension values may (dis)appear from one period of time to the next. The notebook decomposes a ratio, but the logic is also valid for decomposing a sum.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Edit 2&lt;/strong> &amp;ndash; &lt;em>I&amp;rsquo;ve stumbled on &lt;a href="https://medium.com/@shaozhifei/metric-decomposition-formula-to-understand-metric-trend-e693b7a4c8cf">this article&lt;/a> by Shao Zhifei which provides a good derivation of the ratio decomposition formula. I contacted Shao Zhifei on LinkedIn, and he told me they heavily use these formulas at &lt;a href="https://www.grab.com/">Grab&lt;/a>. He also pointed out a typo in the ratio decomposition formula which I have now fixed.&lt;/em>&lt;/p></description></item><item><title>Measuring the carbon footprint of pizzas</title><link>https://maxhalford.github.io/blog/carbon-footprint-pizzas/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/carbon-footprint-pizzas/</guid><description>&lt;p>Making environmentally friendly decisions can only be done with the right information. At Carbonfact, we&amp;rsquo;ve realized a big challenge is the lack of information about industrial processes. We tackle that slowly but surely by gathering data from various sources, and making it available to our customers.&lt;/p>
&lt;p>Regarding food, the French government has a great initiative called &lt;a href="https://agribalyse.ademe.fr/">Agribalyse&lt;/a>. It&amp;rsquo;s a free database of environmental footprints for various food products. It includes raw ingredients straight out from the farm, as well as ready to eat dishes from the supermarket. It&amp;rsquo;s a great initiative, as it allows anyone to do their own research and make informed decisions.&lt;/p></description></item><item><title>Graph components with DuckDB</title><link>https://maxhalford.github.io/blog/graph-components-duckdb/</link><pubDate>Sat, 03 Jun 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/graph-components-duckdb/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Graph problems are quite common. However, it&amp;rsquo;s rare to have access to a database offering graph semantics. There are graph databases, such as &lt;a href="https://neo4j.com/">Neo4j&lt;/a> and &lt;a href="https://spark.apache.org/docs/latest/graphx-programming-guide.html">GraphX&lt;/a>, but it&amp;rsquo;s difficult to justify setting one of those up. One could simply use &lt;a href="https://networkx.org/">networkx&lt;/a> in Python. But that only works if the graph fits in memory.&lt;/p>
&lt;p>From a practical angle, the fact is that people are querying data warehouses in SQL. There are many good reasons to write graph algorithms in SQL. And anyway, one may argue that graphs are a special case of the &lt;a href="https://www.wikiwand.com/en/Relational_model">relational model&lt;/a>.&lt;/p></description></item><item><title>For analytics, don't use dynamic JSON keys</title><link>https://maxhalford.github.io/blog/no-dynamic-keys-in-json/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/no-dynamic-keys-in-json/</guid><description>&lt;p>I love the JSON format. It&amp;rsquo;s the kind of love that grows on you with time. Like others, I&amp;rsquo;ve been using JSON everywhere for so many years, to the point where I just take it for granted.&lt;/p>
&lt;p>I suppose the main thing I like about JSON is its flexibility. You can structure your JSONs without too much care. There will always be a way to consume and manipulate it. But I have discovered a bit of anti-pattern, which I believe is worth raising awareness about. Especially when you&amp;rsquo;re doing analytics.&lt;/p></description></item><item><title>Metric correctness doesn't matter, consistency does</title><link>https://maxhalford.github.io/blog/consistent-metrics/</link><pubDate>Fri, 28 Apr 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/consistent-metrics/</guid><description>&lt;p>&lt;a href="https://www.un.org/en/dayof8billion">According to&lt;/a> the United Nations, the 15th of November &lt;a href="https://www.bbc.co.uk/newsround/63632981">was the day&lt;/a> we crossed 8 billion humans on the planet. How can they be so sure of that? Surely there has to be some margin of error, meaning it could have happened on the 14th or 16th. Then again, does it matter?&lt;/p>
&lt;p>I would argue almost all metrics we look at are incorrect. For instance, I work at a company who&amp;rsquo;s goal is to measure the carbon footprint of clothing items. I can tell you first hand our measurements are stock full of assumptions. In the sustainability world, it&amp;rsquo;s not surprising to get reports like this one:&lt;/p></description></item><item><title>Online gradient descent written in SQL</title><link>https://maxhalford.github.io/blog/ogd-in-sql/</link><pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/ogd-in-sql/</guid><description>&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>this post &lt;a href="https://news.ycombinator.com/item?id=35054786">generated&lt;/a> a few insightful comments on Hacker News. I&amp;rsquo;ve also put the code in a &lt;a href="https://gist.github.com/MaxHalford/823c4e7f9216607dc853724ec74ec692">notebook&lt;/a> for ease of use.&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Modern MLOps is complex because it involves too many components. You need a message bus, a stream processing engine, an API, a model store, a feature store, a monitoring service, etc. Sadly, containerisation software and the unbundling trend have encouraged an appetite for complexity. I believe MLOps shouldn&amp;rsquo;t be this complex. For instance, MLOps can be made simpler by &lt;a href="https://www.ethanrosenthal.com/2022/05/10/database-bundling/">bundling the logic into your database&lt;/a>.&lt;/p></description></item><item><title>Using SymPy in Python doctests</title><link>https://maxhalford.github.io/blog/sympy-doctests/</link><pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sympy-doctests/</guid><description>&lt;p>A program which compiles and runs without errors isn&amp;rsquo;t necessarily correct. I find this to be especially true for statistical software, both as a developer and as a user. Small but nasty bugs creep up on me every week. I keep sane in the membrane by writing many unit tests ðŸ›ðŸ”¨&lt;/p>
&lt;p>I make heavy use of &lt;a href="https://docs.python.org/3/library/doctest.html">doctests&lt;/a>. These are unit tests which you write as Python &lt;a href="https://realpython.com/documenting-python-code/#documenting-your-python-code-base-using-docstrings">docstrings&lt;/a>. They&amp;rsquo;re really handy because they kill two birds with one stone: the unit tests you write for a function also act as documentation.&lt;/p></description></item><item><title>Online active learning in 80 lines of Python</title><link>https://maxhalford.github.io/blog/online-active-learning-river-databutton/</link><pubDate>Sun, 22 Jan 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-active-learning-river-databutton/</guid><description>&lt;p>&lt;a href="https://www.wikiwand.com/en/Active_learning_(machine_learning)">Active learning&lt;/a> is a way to get humans to label data efficiently. A good active learning strategy minimizes the number of necessary labels, while maximizing a model&amp;rsquo;s performance. This usually works by focusing on samples where the model is unsure of its prediction.&lt;/p>
&lt;p>In a batch setting, the model is periodically retrained to learn from the freshly labeled samples. However, the training time is usually too prohibitive for this to happen each time a new label is provided. This isn&amp;rsquo;t the case with online models, because they are able to learn one sample at a time. Active and online learning naturally fit together.&lt;/p></description></item><item><title>Are Airbnb guests less energy efficient than their host?</title><link>https://maxhalford.github.io/blog/airbnb-energy-usage/</link><pubDate>Tue, 17 Jan 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/airbnb-energy-usage/</guid><description>&lt;h2 id="tldr">TLDR&lt;/h2>
&lt;p>I compared the energy consumption of Airbnb guests versus their host, in the same appartment, during 2022. It appears that guests do in fact consume more energy than hosts. The data I used is available to any Airbnb host. I also open-sourced all the code I wrote for this analysis.&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>European energy prices have soared in 2022. It&amp;rsquo;s gone to the point where some Airbnb hosts have become reluctant to rent, believing their guests are too wasteful and cost too much. You can see this by scrolling Airbnb groups on Facebook.&lt;/p></description></item><item><title>The future of River</title><link>https://maxhalford.github.io/blog/future-of-river/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/future-of-river/</guid><description>&lt;div align="center">
&lt;figure >
 &lt;img src="https://maxhalford.github.io/img/blog/future-of-river/tweet.png" style="box-shadow: none;">
 &lt;figcaption>&lt;a href="https://twitter.com/josh_wills/status/1585328751646109696">Source&lt;/a>&lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;p>When I see tweets like this one, I&amp;rsquo;m both happy because people are aware of &lt;a href="https://riverml.xyz/">River&lt;/a>, but also irked because it&amp;rsquo;s really difficult to make production-grade open source software.&lt;/p>
&lt;p>We just had a developer meeting a week ago. We planned &lt;a href="https://github.com/orgs/online-ml/projects/3?query=is%3Aopen+sort%3Aupdated-desc">what we will work on&lt;/a> during the first half of 2023. I thought it would be worthwhile to give a high-level view of how we envision River&amp;rsquo;s future. If not to be comprehensive, at least to reassure potential users that River is alive and kicking ðŸ¤º&lt;/p></description></item><item><title>Parsing garment descriptions with GPT-3</title><link>https://maxhalford.github.io/blog/garment-parsing-gpt3/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/garment-parsing-gpt3/</guid><description>&lt;h2 id="the-task">The task&lt;/h2>
&lt;p>You&amp;rsquo;ll have heard of GPT-3 if you haven&amp;rsquo;t been hiding under a rock. I&amp;rsquo;ve recently been impressed by Nat Friedman &lt;a href="https://twitter.com/natfriedman/status/1575631194032549888">teaching&lt;/a> GPT-3 to use a browser, and SeekWell &lt;a href="https://blog.seekwell.io/gpt3">generating&lt;/a> SQL queries from free-text. I think the most exciting usecases are yet to come. But GPT-3 has a good chance of changing the way we approach mundane tasks at work.&lt;/p>
&lt;p>I wrote an &lt;a href="https://maxhalford.github.io/blog/carbonfact-nlp-open-problem">article&lt;/a> a couple of months ago about a boring task I have to do at work. I got a few interesting suggestions by email. RaphaÃ«l suggested a &lt;a href="https://huggingface.co/tasks/question-answering">question-answering&lt;/a> model &lt;a href="https://raphaelsty.github.io/blog/qa/">here&lt;/a>. I&amp;rsquo;ve slowly become convinced that GPT-3 might be the perfect tool for the job. It&amp;rsquo;s cold and miserable where I am, so I thought it would be opportune to take GPT-3 for a spin ðŸ§™&lt;/p></description></item><item><title>Dynamic on-screen TV keyboards</title><link>https://maxhalford.github.io/blog/dynamic-on-screen-keyboards/</link><pubDate>Sun, 25 Sep 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/dynamic-on-screen-keyboards/</guid><description>&lt;p>&lt;em>This article has some interactive keyboards, therefore I recommend reading it from your computer rather than your phone.&lt;/em>&lt;/p>
&lt;h2 id="on-screen-tv-keyboards">On-screen TV keyboards&lt;/h2>
&lt;p>I&amp;rsquo;ve recently been spending time at my brother&amp;rsquo;s place. We usually eat in front of TV. I&amp;rsquo;ve thus found myself typing stuff on the Netflix/Amazon/Plex TV apps. The typing happens through a remote controller, which is slower than typing with ones fingers. However, the TV apps usually suggest the correct show/movie after five or six keystrokes, so it&amp;rsquo;s not that bad.&lt;/p></description></item><item><title>NLP at Carbonfact: how would you do it?</title><link>https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/</guid><description>&lt;h2 id="the-task">The task&lt;/h2>
&lt;p>I work at a company called &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a>. Our core value proposal is computing the &lt;a href="https://www.wikiwand.com/en/Carbon_footprint">carbon footprint&lt;/a> of clothing items, expressed in &lt;a href="https://www.wikiwand.com/en/Carbon_Dioxide_Equivalent">carbon dioxide equivalent&lt;/a> &amp;ndash; $kgCO_2e$ in short. For instance, we started by measuring the footprint of shoes &amp;ndash; no pun intended. We do these measurements with &lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment">life cycle analysis (LCA)&lt;/a> software we built ourselves. We use these analyses to fuel higher-level tasks for our clients, such as &lt;a href="https://www.wikiwand.com/en/Carbon_accounting">carbon accounting&lt;/a> and &lt;a href="https://www.wikiwand.com/en/Sustainable_procurement">sustainable procurement&lt;/a>.&lt;/p>
&lt;p>A life cycle analysis is essentially a recipe, the output of which is a carbon footprint assessment. Like any recipe, an LCA necessitates ingredients. In a &lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment#/Cradle-to-gate">cradle-to-gate&lt;/a> scenario, this includes everything that is needed to make the product: the materials, the mass, the manufacturing methods, the transport between factories, etc. In our experience, the biggest impact on a product&amp;rsquo;s footprint come from the materials which it is made of.&lt;/p></description></item><item><title>Matrix inverse mini-batch updates</title><link>https://maxhalford.github.io/blog/matrix-inverse-mini-batch/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/matrix-inverse-mini-batch/</guid><description>&lt;p>The inverse covariance matrix, also called &lt;a href="https://www.wikiwand.com/en/Precision_matrix">precision matrix&lt;/a>, is useful in many places across the field of statistics. For instance, in machine learning, it is used for &lt;a href="https://maxhalford.github.io/blog/bayesian-linear-regression">Bayesian regression&lt;/a> and &lt;a href="https://scikit-learn.org/stable/modules/mixture.html#gmm">mixture modelling&lt;/a>.&lt;/p>
&lt;p>What&amp;rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn&amp;rsquo;s &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope">elliptic envelope&lt;/a> method could have an online variant with a &lt;code>partial_fit&lt;/code> method.&lt;/p></description></item><item><title>A rant against dbt ref</title><link>https://maxhalford.github.io/blog/dbt-ref-rant/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/dbt-ref-rant/</guid><description>&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>&lt;em>Let me be absolutely clear: I think dbt is a great tool. Although this post is a rant, the goal is to be constructive and suggest an improvement.&lt;/em>&lt;/p>
&lt;h2 id="dbt-in-a-nutshell">dbt in a nutshell&lt;/h2>
&lt;p>&lt;a href="https://www.getdbt.com/">dbt&lt;/a> is a workflow orchestrator for SQL. In other words, it&amp;rsquo;s a fancy &lt;a href="https://www.wikiwand.com/en/Make_(software)">Make&lt;/a> for data analytics. What makes dbt special is that it is the first workflow orchestrator that is dedicated to the SQL language. It said out loud what many data teams were thinking: you can get a lot done with SQL.&lt;/p></description></item><item><title>First IRL meetup with the River developers</title><link>https://maxhalford.github.io/blog/first-river-meetup/</link><pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/first-river-meetup/</guid><description>&lt;p>&lt;a href="https://github.com/online-ml/river/">River&lt;/a> is a Python software for doing online machine learning. It&amp;rsquo;s the result of a merger in early 2020 between &lt;a href="https://github.com/online-ml/river">creme&lt;/a> and &lt;a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow&lt;/a>. &lt;a href="https://smastelini.github.io/">Saulo Mastelini&lt;/a>, &lt;a href="https://jacobmontiel.github.io/">Jacob Montiel&lt;/a>, and myself are the three core developers. But there are many more people who contribute here and there!&lt;/p>
&lt;p>This week Saulo Mastelini and I got to meet in person. This is worth mentioning because Saulo is originally from Brazil, whereas I&amp;rsquo;m based in Europe. We connected and I&amp;rsquo;m glad to think of him as a good friend from now on. Of course we were not alone: some friends of mine from university also joined the fun. These are people who initially contributed to creme, back in what we already call the old days! Each one of them has their own areas of expertise, and contributed to various parts of the codebase.&lt;/p></description></item><item><title>Online machine learning with River @ GAIA</title><link>https://maxhalford.github.io/blog/online-machine-learning-with-river/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-with-river/</guid><description/></item><item><title>Fuzzy regex matching in Python</title><link>https://maxhalford.github.io/blog/fuzzy-regex-matching-in-python/</link><pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fuzzy-regex-matching-in-python/</guid><description>&lt;h2 id="fuzzy-string-matching-in-a-nutshell">Fuzzy string matching in a nutshell&lt;/h2>
&lt;p>Say we&amp;rsquo;re looking for a pattern in a blob of text. If you know the text has no typos, then determining whether it contains a pattern is trivial. In Python you can use the &lt;code>in&lt;/code> function. You can also write a regex pattern with the &lt;code>re&lt;/code> module from the standard library. But what about if the text contains typos? For instance, this might be the case with user inputs on a website, or with OCR outputs. This is a much harder problem.&lt;/p></description></item><item><title>OCR spelling correction is hard</title><link>https://maxhalford.github.io/blog/ocr-spelling-correction-is-hard/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/ocr-spelling-correction-is-hard/</guid><description>&lt;p>I recently saw &lt;a href="https://news.ycombinator.com/item?id=30576435">SymSpell&lt;/a> pop up on Hackernews. It claims to be a million times faster than &lt;a href="https://norvig.com/spell-correct.html">Peter Norvig&amp;rsquo;s spelling corrector&lt;/a>. I think it&amp;rsquo;s great that there&amp;rsquo;s a fast open source solution for spelling correction. But in my experience, the most challenging aspect of spelling correction is not necessarily speed.&lt;/p>
&lt;p>When I &lt;a href="https://maxhalford.github.io/blog/one-year-at-alan">worked at Alan&lt;/a>, I mostly wrote logic to extract structured information from medical documents. After some months working on the topic, I have to admit I hadn&amp;rsquo;t cracked the problem. The goal was to process &amp;gt;80% of documents with no human interaction, but when I left we had only reached 35%. However, I developed a good understanding of what made this task so difficult.&lt;/p></description></item><item><title>Comic book panel segmentation</title><link>https://maxhalford.github.io/blog/comic-book-panel-segmentation/</link><pubDate>Sat, 05 Mar 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/comic-book-panel-segmentation/</guid><description>&lt;p>&lt;strong>Edit (2023-05-26)&lt;/strong> &amp;ndash; &lt;em>I&amp;rsquo;ve learnt about the &lt;a href="https://github.com/njean42/kumiko">Kumiko project&lt;/a>, which is exactly devoted to slicing comic book panels. There&amp;rsquo;s even a live &lt;a href="https://kumiko.njean.me/demo">tool&lt;/a>. I discovered it thanks to being pinged on &lt;a href="https://github.com/njean42/kumiko/issues/12">this&lt;/a> issue.&lt;/em>&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>I&amp;rsquo;ve recently been reading some comic books I used to devour as a kid. Especially those from the golden era of francophone comics: Thorgal, Lanfeust, XIII, Tintin, Largo Winch, Blacksad, Aldebaran, etc.&lt;/p>
&lt;p>It&amp;rsquo;s not easy to get my hands on many of them. Luckily enough I found a website called &lt;a href="https://readcomiconline.li/">ReadComicOnline&lt;/a> which is delightfully profuse. It gives access to comics for free under the murky &amp;ldquo;fair use&amp;rdquo; copyright doctrine. I&amp;rsquo;m very doubtful about the legality of the website, but I&amp;rsquo;m still using it for lack of a better option.&lt;/p></description></item><item><title>Online machine learning in practice @ PyData PDX</title><link>https://maxhalford.github.io/blog/online-machine-learning-in-practice-pydata-pdx/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-in-practice-pydata-pdx/</guid><description/></item><item><title>The online machine learning predict/fit switcheroo</title><link>https://maxhalford.github.io/blog/predict-fit-switcheroo/</link><pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/predict-fit-switcheroo/</guid><description>&lt;h2 id="why-im-writing-this">Why I&amp;rsquo;m writing this&lt;/h2>
&lt;p>Fact: designing open source software is hard. It&amp;rsquo;s difficult to make design decisions which don&amp;rsquo;t make any compromises. I like to fall back on Dieter Rams&amp;rsquo; &lt;a href="https://ifworlddesignguide.com/design-specials/dieter-rams-10-principles-for-good-design">10 principles for good design&lt;/a>. I feel like they apply rather well to software design. Especially when said software is open source, due to the many users and the plethora of use cases.&lt;/p>
&lt;p>I had to make a significant design decision for &lt;a href="https://github.com/online-ml/river/">River&lt;/a>. It boils down to the fact that making a prediction with a model pipeline is a stateful operation, whereas users understandably expect it to be pure with no side-effects. This regularly comes up on the issue tracker, as you can see &lt;a href="https://github.com/online-ml/river/issues/130">here&lt;/a>, &lt;a href="https://github.com/online-ml/river/issues/359">here&lt;/a>, and &lt;a href="https://github.com/online-ml/river/issues/499">here&lt;/a>.&lt;/p></description></item><item><title>Weighted sampling without replacement in pure Python</title><link>https://maxhalford.github.io/blog/weighted-sampling-without-replacement/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/weighted-sampling-without-replacement/</guid><description>&lt;p>I&amp;rsquo;m working on a problem where I need to sample &lt;code>k&lt;/code> items from a list without replacement. The sampling has to be weighted. In Python, &lt;code>numpy&lt;/code> has &lt;code>random.choice&lt;/code> method which allows doing this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">42&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">population&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dirichlet&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">population&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">choice&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">population&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">replace&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">weights&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">array&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">9&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I&amp;rsquo;m always wary of using &lt;code>numpy&lt;/code> without thinking because I know it incurs some overhead. This overhead is usually meaningful when small amounts of data are involved. In such a case, a pure Python implementation may be faster.&lt;/p></description></item><item><title>Online machine learning in practice @ Applied AI</title><link>https://maxhalford.github.io/blog/real-time-ml-next-frontier-applied-ai/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/real-time-ml-next-frontier-applied-ai/</guid><description/></item><item><title>Online machine learning in practice @ LVMH</title><link>https://maxhalford.github.io/blog/real-time-ml-next-frontier-lvmh/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/real-time-ml-next-frontier-lvmh/</guid><description/></item><item><title>Web scraping, upside down</title><link>https://maxhalford.github.io/blog/declarative-web-scraping/</link><pubDate>Thu, 11 Nov 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/declarative-web-scraping/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Web scraping is the art of extracting information from web pages. A web page is essentially an amalgamation of HTML tags. Usually, we&amp;rsquo;re looking for a particular piece of information on a given web page. This may be done by fetching the HTML content of the page in question, and then running some HTML parsing logic. It&amp;rsquo;s quite straightforward.&lt;/p>
&lt;p>There are many tools in the wild to perform web scraping. For instance, in Python, you may use &lt;a href="https://docs.python-requests.org/en/latest/">requests&lt;/a> in combination with &lt;a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup&lt;/a>. You can also automate some of the more mundane aspects of scraping by using &lt;a href="https://scrapy.org/">Scrapy&lt;/a>.&lt;/p></description></item><item><title>One year at Alan</title><link>https://maxhalford.github.io/blog/one-year-at-alan/</link><pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/one-year-at-alan/</guid><description>&lt;h2 id="context">Context&lt;/h2>
&lt;p>Today marks the 1 year anniversary since I started working at &lt;a href="https://alan.com/">Alan&lt;/a>. It&amp;rsquo;s my first real job, and certainly the place where I grew up the most professionally. I&amp;rsquo;m writing this post to summarise what I did and what I learnt at Alan.&lt;/p>
&lt;p>Alan is a special company. It has a unique culture that is starting to become famous in France. I won&amp;rsquo;t expand on the way things work at Alan, and will simply focus on the way I experienced it. Let me just say this: it works. The pace at which stuff gets shipped is insane. And yet, it&amp;rsquo;s a healthy environment to be working in. Alaners are some of the kindest and wisest human beings I&amp;rsquo;ve had the chance to meet.&lt;/p></description></item><item><title>Manipulating ephemeral data with git</title><link>https://maxhalford.github.io/blog/manipulating-ephemeral-data-with-git/</link><pubDate>Thu, 07 Oct 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/manipulating-ephemeral-data-with-git/</guid><description/></item><item><title>Dashboards and GROUPING SETS</title><link>https://maxhalford.github.io/blog/grouping-sets/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/grouping-sets/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>At &lt;a href="https://alan.com/">Alan&lt;/a>, we do almost all our data analysis in SQL. Our data warehouse used to be &lt;a href="https://www.postgresql.org/">PostgreSQL&lt;/a>, and have since switched to &lt;a href="https://www.snowflake.com/">Snowflake&lt;/a> for performance reasons. We load data into our warehouse with &lt;a href="https://airflow.apache.org/">Airflow&lt;/a>. This includes dumps of our production database, third-party data, and health data from other actors in the health ecosystem. This is raw data. We transform this into prepared data via an in-house tool that resembles &lt;a href="https://www.getdbt.com/">dbt&lt;/a>. You can read more about it &lt;a href="https://medium.com/alan/how-we-solve-the-problem-of-sharing-actionable-data-with-the-team-7e4afeff3cac">here&lt;/a>.&lt;/p></description></item><item><title>Homoglyphs: different characters that look identical</title><link>https://maxhalford.github.io/blog/homoglyphs/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/homoglyphs/</guid><description>&lt;h2 id="a-wild-homoglyph-appears">A wild homoglyph appears&lt;/h2>
&lt;p>For instance, can you tell if there&amp;rsquo;s a difference between &lt;code>H&lt;/code> and &lt;code>Î—&lt;/code>? How about &lt;code>N&lt;/code> and &lt;code>Î&lt;/code>? These characters may seem identical, but they are actually different. You can try this out for yourself in Python:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="s1">&amp;#39;H&amp;#39;&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s1">&amp;#39;Î—&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kc">False&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="s1">&amp;#39;N&amp;#39;&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s1">&amp;#39;Î&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kc">False&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Indeed, these all represent different Unicode characters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;H&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Î—&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="mi">72&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">919&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;N&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Î&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="mi">78&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">925&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>Î—&lt;/code> in fact represents the capital &lt;a href="https://www.wikiwand.com/en/Eta">Eta&lt;/a> letter, while &lt;code>Î&lt;/code> is a capital &lt;a href="https://www.wikiwand.com/en/Nu_(letter)">Nu&lt;/a>. In fact, entering &lt;code>H&lt;/code> or &lt;code>Î—&lt;/code> in Google will produce different results. The same goes for &lt;code>N&lt;/code> and &lt;code>Î&lt;/code>.&lt;/p></description></item><item><title>Automated document processing at Alan</title><link>https://maxhalford.github.io/blog/medium-document-processing/</link><pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/medium-document-processing/</guid><description/></item><item><title>Text classification by data compression</title><link>https://maxhalford.github.io/blog/text-classification-by-compression/</link><pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/text-classification-by-compression/</guid><description>&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>I posted this &lt;a href="https://news.ycombinator.com/item?id=27440093">on Hackernews&lt;/a> and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&amp;rsquo;s also some insightful references to data compression theory and its ties to statistical learning&lt;/em>&lt;/p>
&lt;p>Last night I felt like reading &lt;a href="http://aima.cs.berkeley.edu/">&lt;em>Artificial Intelligence: A Modern Approach&lt;/em>&lt;/a>. I stumbled on something fun in the natural language processing chapter. The section I was reading dealt with classifying text. The idea of the particular subsection I was reading was to classify documents by using a &lt;a href="https://www.wikiwand.com/en/Data_compression">compression algorithm&lt;/a>. This is such a left field idea, and yet it does make sense when you think about it. To quote the book:&lt;/p></description></item><item><title>Reducing the memory footprint of a scikit-learn text classifier</title><link>https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/</link><pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/</guid><description>&lt;h2 id="context">Context&lt;/h2>
&lt;p>This week at Alan I&amp;rsquo;ve been working on parsing &lt;a href="https://www.wikiwand.com/fr/Ordonnance_(m%C3%A9decine)">French medical prescriptions&lt;/a>. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image. We can thus use the text transcription to classify the prescription.&lt;/p></description></item><item><title>An overview of dataset time travel</title><link>https://maxhalford.github.io/blog/dataset-time-travel/</link><pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/dataset-time-travel/</guid><description>&lt;h2 id="tldr">TLDR&lt;/h2>
&lt;p>You&amp;rsquo;re a data scientist. The engineers in your company overwrite data in the production database. You want to access overwritten data to train your models. How?&lt;/p>
&lt;h2 id="i-thought-time-travel-only-existed-in-the-movies">I thought time travel only existed in the movies&lt;/h2>
&lt;p>You&amp;rsquo;re probably right, expect maybe for &lt;a href="https://www.wikiwand.com/en/Time_travel_claims_and_urban_legends#/Present-day_hipster_at_1941_bridge_opening">this guy&lt;/a>.&lt;/p>
&lt;p>I want to discuss a concept that&amp;rsquo;s been on my mind for a while now. I like to call it &amp;ldquo;dataset time travel&amp;rdquo; because it has a nice ring to it. But the association of &amp;ldquo;time travel&amp;rdquo; and &amp;ldquo;data&amp;rdquo; has already been used elsewhere. It&amp;rsquo;s not something I&amp;rsquo;m pulling out from thin air. Essentially, what I want to discuss is the ability to view a dataset at any given point in the past. Having this ability is powerful, as it allows answering important business questions. As an example, let&amp;rsquo;s say we have a database table called &lt;code>users&lt;/code>. We might ask the following question:&lt;/p></description></item><item><title>The challenges of online machine learning in production @ ItaÃº Unibanco</title><link>https://maxhalford.github.io/blog/challenges-of-online-machine-learning-in-production/</link><pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/challenges-of-online-machine-learning-in-production/</guid><description/></item><item><title>Quelle est lâ€™empreinte Ã©cologique du Big Data? @ Toulouse Tech</title><link>https://maxhalford.github.io/blog/empreinte-ecologie-du-big-data/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/empreinte-ecologie-du-big-data/</guid><description/></item><item><title>Organising a Kaggle InClass competition with a fairness metric</title><link>https://maxhalford.github.io/blog/fairness-competition/</link><pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fairness-competition/</guid><description>&lt;h2 id="some-context">Some context&lt;/h2>
&lt;p>I co-organised a data science competition during the second half of 2020. This was in fact the 5th edition of the &amp;ldquo;DÃ©fi IA&amp;rdquo;, which is a recurring event that happens on a yearly basis. It is essentially a supervised machine learning competition for students from French speaking universities and engineering schools. This year was the first time that Kaggle was used to host the competition. Before that we used a custom platform that I wrote during my student years. You can read more about this &lt;a href="https://maxhalford.github.io/blog/openbikes-challenge">here&lt;/a>.&lt;/p></description></item><item><title>Converting Amazon Textract tables to pandas DataFrames</title><link>https://maxhalford.github.io/blog/textract-table-to-pandas/</link><pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/textract-table-to-pandas/</guid><description>&lt;p>I&amp;rsquo;m currently doing a lot of document processing at work. One of my tasks is to extract tables from PDF files. I evaluated &lt;a href="https://aws.amazon.com/textract/?nc1=h_ls">Amazon Textract&lt;/a>&amp;rsquo;s &lt;a href="https://docs.aws.amazon.com/textract/latest/dg/how-it-works-tables.html">table extraction&lt;/a> capability as part of this task. It&amp;rsquo;s very well documented, as is the rest of Textract. I was slightly disappointed by &lt;a href="https://docs.aws.amazon.com/textract/latest/dg/examples-blocks.html">the examples&lt;/a>, but nothing serious.&lt;/p>
&lt;p>I wanted to write this short blog post to share a piece of code I use to convert tables extracted through Amazon Textract to &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html">&lt;code>pandas.DataFrame&lt;/code>&lt;/a>s. I&amp;rsquo;ll be using the following anonymised image as an example:&lt;/p></description></item><item><title>What my PhD was about</title><link>https://maxhalford.github.io/blog/phd-about/</link><pubDate>Wed, 06 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/phd-about/</guid><description>&lt;p>I defended my PhD thesis on the 12th of October 2020, exactly 3 years and 11 days after having started it. The title of my PhD is &lt;em>Machine learning for query selectivity estimation in relational databases&lt;/em>. I thought it would be worthwhile to summarise what I did. Note sure anyone will read this, but at least I&amp;rsquo;ll be able to remember what I did when I grow old and senile.&lt;/p></description></item><item><title>Computing cross-correlations in SQL</title><link>https://maxhalford.github.io/blog/sql-cross-correlations/</link><pubDate>Tue, 17 Nov 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sql-cross-correlations/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I&amp;rsquo;m currently working on a problem at work where I have to measure the impact of a &lt;span style="color: SlateBlue;">growth initiative&lt;/span> on a &lt;span style="color: MediumSeaGreen;">performance metric&lt;/span>. Hypothetically, this might to answer the following kind of question:&lt;/p>
&lt;blockquote>
&lt;p>I&amp;rsquo;ve spent &lt;span style="color: SlateBlue;">X amount of money&lt;/span>, what is the impact on the &lt;span style="color: MediumSeaGreen;">number of visitors on my website&lt;/span>?&lt;/p>&lt;/blockquote>
&lt;p>Of course, there are many measures that can be taken to answer such a question. I decided to measure the correlation between the &lt;span style="color: SlateBlue;">initiative&lt;/span> and the &lt;span style="color: MediumSeaGreen;">metric&lt;/span>, with the latter being shifted forward in time. This measure is called the &lt;a href="https://www.wikiwand.com/en/Cross-correlation">cross-correlation&lt;/a>. It&amp;rsquo;s different from &lt;a href="https://www.wikiwand.com/en/Autocorrelation">serial correlation&lt;/a>, which is the correlation of a series with a shifted version of itself.&lt;/p></description></item><item><title>Unsupervised text classification with word embeddings</title><link>https://maxhalford.github.io/blog/unsupervised-text-classification/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/unsupervised-text-classification/</guid><description>&lt;div align="center" >
 &lt;img height="300px" src="https://maxhalford.github.io/img/blog/document-classification/morpheus.jpg" alt="morpheus">
 &lt;br>
&lt;/div>
&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>since writing this article, I have discovered that the method I describe is a form of &lt;a href="https://www.wikiwand.com/en/Zero-shot_learning">zero-shot learning&lt;/a>. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>I stumbled on a &lt;a href="https://www.aclweb.org/anthology/P19-1036/">paper&lt;/a> entitled &amp;ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings&amp;rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out. Note that they call the &lt;code>tech -&amp;gt; technology&lt;/code> trick &amp;ldquo;label enrichment&amp;rdquo;.&lt;/em>&lt;/p></description></item><item><title>Focal loss implementation for LightGBM</title><link>https://maxhalford.github.io/blog/lightgbm-focal-loss/</link><pubDate>Sun, 20 Sep 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/lightgbm-focal-loss/</guid><description>&lt;p>&lt;strong>Edit (2021-01-26)&lt;/strong> &amp;ndash; &lt;em>I initially wrote this blog post using version 2.3.1 of LightGBM. I&amp;rsquo;ve now updated it to use version 3.1.1. There are a couple of subtle but important differences between version 2.x.y and 3.x.y. If you&amp;rsquo;re using version 2.x.y, then I strongly recommend you to upgrade to version 3.x.y.&lt;/em>&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>If you&amp;rsquo;re reading this blog post, then you&amp;rsquo;re likely to be aware of &lt;a href="https://github.com/microsoft/LightGBM">LightGBM&lt;/a>. The latter is a best of breed &lt;a href="https://explained.ai/gradient-boosting/">gradient boosting&lt;/a> library. As of 2020, it&amp;rsquo;s still the go-to machine learning model for tabular data. It&amp;rsquo;s also ubiquitous in competitive machine learning.&lt;/p></description></item><item><title>A few intermediate pandas tricks</title><link>https://maxhalford.github.io/blog/pandas-tricks/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/pandas-tricks/</guid><description>&lt;p>I want to use this post to share some &lt;code>pandas&lt;/code> snippets that I find useful. I use them from time to time, in particular when I&amp;rsquo;m doing &lt;a href="https://www.kaggle.com/search?q=time+series+in%3Acompetitions">time series competitions&lt;/a> on platforms such as Kaggle. Like any data scientist, I perform similar data processing steps on different datasets. Usually, I put repetitive patterns in &lt;a href="https://github.com/MaxHalford/xam">&lt;code>xam&lt;/code>&lt;/a>, which is my personal data science toolbox. However, I think that the following snippets are too small and too specific for being added into a library.&lt;/p></description></item><item><title>A brief introduction to online machine learning @ Hong Kong Machine Learning Meetup</title><link>https://maxhalford.github.io/blog/brief-introduction-to-online-machine-learning/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/brief-introduction-to-online-machine-learning/</guid><description/></item><item><title>The correct way to evaluate online machine learning models</title><link>https://maxhalford.github.io/blog/online-learning-evaluation/</link><pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-learning-evaluation/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Most supervised machine learning algorithms work in the batch setting, whereby they are fitted on a training set offline, and are used to predict the outcomes of new samples. The only way for batch machine learning algorithms to learn from new samples is to train them from scratch with both the old samples and the new ones. Meanwhile, some learning algorithms are online, and can predict as well as update themselves when new samples are available. This encompasses any model trained with &lt;a href="https://leon.bottou.org/publications/pdf/compstat-2010.pdf">stochastic gradient descent&lt;/a> &amp;ndash; which includes deep neural networks, &lt;a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">factorisation machines&lt;/a>, and &lt;a href="https://www.cs.huji.ac.il/~shais/papers/ShalevSiSrCo10.pdf">SVMs&lt;/a> &amp;ndash; as well as &lt;a href="https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf">decision trees&lt;/a>, &lt;a href="https://ai.stanford.edu/~ang/papers/icml04-onlinemetric.pdf">metric learning&lt;/a>, and &lt;a href="https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf">naÃ¯ve Bayes&lt;/a>.&lt;/p></description></item><item><title>Online machine learning with decision trees @ Toulouse AOC workgroup</title><link>https://maxhalford.github.io/blog/online-machine-learning-with-decision-trees/</link><pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-with-decision-trees/</guid><description/></item><item><title>Server-sent events in Flask without extra dependencies</title><link>https://maxhalford.github.io/blog/flask-sse-no-deps/</link><pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/flask-sse-no-deps/</guid><description>&lt;p>&lt;a href="https://www.wikiwand.com/en/Server-sent_events">Server-sent events (SSE)&lt;/a> is a mechanism for sending updates from a server to a client. The fundamental difference with &lt;a href="https://www.wikiwand.com/en/WebSocket">WebSockets&lt;/a> is that the communication only goes in one direction. In other words, the client cannot send information to the server. For many usecases this is all you might need. Indeed, if you just want to receive notifications/updates/messages, then using a WebSocket is overkill. Once you&amp;rsquo;ve implemented the SSE functionality on your server, then all you need on a JavaScript client is an &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/EventSource">&lt;code>EventSource&lt;/code>&lt;/a>. Trust me, it&amp;rsquo;s very straightforward.&lt;/p></description></item><item><title>I got plagiarized and Google didn't help</title><link>https://maxhalford.github.io/blog/plagiarism-google-didnt-help/</link><pubDate>Fri, 17 Apr 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/plagiarism-google-didnt-help/</guid><description>&lt;p>One of my most popular articles is &lt;a href="https://maxhalford.github.io/blog/target-encoding">the one on target encoding&lt;/a>. It gets a fair amount of mentions on Kaggle discussions and I see it pop up from time to time in other contexts. It also brings me around 2500 unique monthly viewers. That&amp;rsquo;s quite a chunk of people for an unambitious blogger like me. Up to a few months ago, my article was on the first page of Google when you typed in searches such as &amp;ldquo;&lt;em>target encoding python&lt;/em>&amp;rdquo; and &amp;ldquo;&lt;em>bayesian target encoding&lt;/em>&amp;rdquo;. This was purely organic and it felt nice to have a relevant article, even though that&amp;rsquo;s not the main reason why I blog.&lt;/p></description></item><item><title>Our solution to the IDAO 2020 qualifiers</title><link>https://maxhalford.github.io/blog/idao-2020-qualifiers-solution/</link><pubDate>Sun, 12 Apr 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/idao-2020-qualifiers-solution/</guid><description/></item><item><title>Speeding up scikit-learn for single predictions</title><link>https://maxhalford.github.io/blog/speeding-up-sklearn-single-predictions/</link><pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/speeding-up-sklearn-single-predictions/</guid><description>&lt;p>It is now common practice to train machine learning models offline before putting them behind an API endpoint to serve predictions. Specifically, we want an API route which can make a prediction for a single row/instance/sample/data point/individual (&lt;a href="https://www.youtube.com/watch?v=1prhCWO_518">call it what you want&lt;/a>). Nowadays, we have great tools to do this that care of the nitty-gritty details, such as &lt;a href="https://github.com/cortexlabs/cortex">Cortex&lt;/a>, &lt;a href="https://www.mlflow.org/docs/latest/models.html">MLFlow&lt;/a>, &lt;a href="https://www.kubeflow.org/docs/components/serving/">Kubeflow&lt;/a>, and &lt;a href="https://github.com/ucbrise/clipper">Clipper&lt;/a>. There are also paid services that hold your hand a bit more, such as &lt;a href="https://www.datarobot.com/">DataRobot&lt;/a>, &lt;a href="https://www.h2o.ai/">H2O&lt;/a>, and &lt;a href="https://www.cubonacci.com/">Cubonacci&lt;/a>. One could argue that deploying machine learning models has never been easier.&lt;/p></description></item><item><title>Machine learning for streaming data with creme</title><link>https://maxhalford.github.io/blog/medium-creme/</link><pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/medium-creme/</guid><description/></item><item><title>Global explanation of machine learning with sensitivity analysis @ MASCOT-NUM</title><link>https://maxhalford.github.io/blog/global-explanation-of-ml-with-sensitivity-analysis/</link><pubDate>Tue, 10 Mar 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/global-explanation-of-ml-with-sensitivity-analysis/</guid><description/></item><item><title>Bayesian linear regression for practitioners</title><link>https://maxhalford.github.io/blog/bayesian-linear-regression/</link><pubDate>Wed, 26 Feb 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/bayesian-linear-regression/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Suppose you have an infinite stream of feature vectors $x_i$ and targets $y_i$. In this case, $i$ denotes the order in which the data arrives. If you&amp;rsquo;re doing supervised learning, then your goal is to estimate $y_i$ &lt;em>before&lt;/em> it is revealed to you. In order to do so, you have a model which is composed of parameters denoted $\theta_i$. For instance, $\theta_i$ represents the feature weights when using linear regression. After a while, $y_i$ will be revealed, which will allow you to update $\theta_i$ and thus obtain $\theta_{i+1}$. To perform the update, you may apply whichever learning rule you wish &amp;ndash; for instance most people use &lt;a href="https://www.wikiwand.com/en/Stochastic_gradient_descent#/Extensions_and_variants">some flavor of stochastic gradient descent&lt;/a>. The process I just described is called &lt;a href="https://www.wikiwand.com/en/Online_machine_learning">online supervised machine learning&lt;/a>. The difference between online machine learning and the more traditional batch machine learning is that an online model is dynamic and learns on the fly. Online learning solves a lot of pain points in real-world environments, mostly because it doesn&amp;rsquo;t require retraining models from scratch every time new data arrives.&lt;/p></description></item><item><title>Under-sampling a dataset with desired ratios</title><link>https://maxhalford.github.io/blog/undersampling-ratios/</link><pubDate>Tue, 17 Dec 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/undersampling-ratios/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I&amp;rsquo;ve just spent a few hours looking at under-sampling and how it can help a classifier learn from an imbalanced dataset. The idea is quite simple: randomly sample the majority class and leave the minority class untouched. There are more sophisticated ways to do this &amp;ndash; for instance by creating synthetic observations from the minority class &lt;em>Ã  la&lt;/em> &lt;a href="http://rikunert.com/SMOTE_explained">SMOTE&lt;/a> &amp;ndash; but I won&amp;rsquo;t be discussing that here.&lt;/p>
&lt;p>I checked out the &lt;a href="https://imbalanced-learn.readthedocs.io/en/stable/index.html">&lt;code>imblearn&lt;/code>&lt;/a> library and noticed they have an implementation of random under-sampling aptly named &lt;a href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler">&lt;code>RandomUnderSampler&lt;/code>&lt;/a>. It contains a &lt;code>sampling_strategy&lt;/code> parameter which gives some control over the sampling. By the default the observations are resampled so that each class is equally represented:&lt;/p></description></item><item><title>The benefits of online machine learning @ Quantmetry</title><link>https://maxhalford.github.io/blog/the-benefits-of-online-learning-quantmetry/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/the-benefits-of-online-learning-quantmetry/</guid><description/></item><item><title>The benefits of online machine learning @ Element AI</title><link>https://maxhalford.github.io/blog/the-benefits-of-online-learning-element-ai/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/the-benefits-of-online-learning-element-ai/</guid><description/></item><item><title>Finding fuzzy duplicates with pandas</title><link>https://maxhalford.github.io/blog/transitive-duplicates/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/transitive-duplicates/</guid><description>&lt;p>Duplicate detection is the task of finding two or more instances in a dataset that are in fact identical. As an example, take the following toy dataset:&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">&lt;/th>
 &lt;th style="text-align: center">&lt;strong>First name&lt;/strong>&lt;/th>
 &lt;th style="text-align: center">&lt;strong>Last name&lt;/strong>&lt;/th>
 &lt;th style="text-align: center">&lt;strong>Email&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">0&lt;/td>
 &lt;td style="text-align: center">Erlich&lt;/td>
 &lt;td style="text-align: center">Bachman&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.com">eb@piedpiper.com&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">1&lt;/td>
 &lt;td style="text-align: center">Erlich&lt;/td>
 &lt;td style="text-align: center">Bachmann&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.com">eb@piedpiper.com&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">Erlik&lt;/td>
 &lt;td style="text-align: center">Bachman&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.co">eb@piedpiper.co&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">Erlich&lt;/td>
 &lt;td style="text-align: center">Bachmann&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.com">eb@piedpiper.com&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>Each of these instances (rows, if you prefer) corresponds to the same &amp;ldquo;thing&amp;rdquo; &amp;ndash; note that I&amp;rsquo;m not using the word &amp;ldquo;entity&amp;rdquo; because &lt;a href="https://www.wikiwand.com/en/Record_linkage#/Entity_resolution">entity resolution&lt;/a> is a different, and yet related, concept. In my experience there are two main reasons why data duplication may occur:&lt;/p></description></item><item><title>A smooth approach to putting machine learning into production</title><link>https://maxhalford.github.io/blog/machine-learning-production/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/machine-learning-production/</guid><description>&lt;p>Putting machine learning into production is hard. Usually I&amp;rsquo;m doubtful of such statements, but in this case I&amp;rsquo;ve never met anyone for whom everything has gone smoothly. Most data scientists might agree that there is a huge gap between their local environment and a live environment. In fact, &amp;ldquo;productionalizing&amp;rdquo; machine learning is such a complex topic that entire companies have risen to address the issue. I&amp;rsquo;m not just talking about running a gigantic grid search and finding the best model, I&amp;rsquo;m talking about putting a machine learning model live so that it actually has a positive impact on your business/project. Off the top of my head: &lt;a href="https://www.cubonacci.com/">Cubonacci&lt;/a>, &lt;a href="https://www.h2o.ai/">H2O&lt;/a>, &lt;a href="https://cloud.google.com/automl/">Google AutoML&lt;/a>, &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html">Amazon Sagemaker&lt;/a>, and &lt;a href="https://www.datarobot.com/">DataRobot&lt;/a>. In other words people are making money off businesses because data scientists and engineers are having a hard putting their models into production. In my opinion if a data scientist can&amp;rsquo;t put her model into production herself then something is wrong. Life should be simpler.&lt;/p></description></item><item><title>The benefits of online machine learning @ Airbus Bizlab</title><link>https://maxhalford.github.io/blog/the-benefits-of-online-learning-airbus-bizlab/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/the-benefits-of-online-learning-airbus-bizlab/</guid><description/></item><item><title>Machine learning incrÃ©mental: des concepts Ã  la pratique @ Toulouse Data Science Meetup</title><link>https://maxhalford.github.io/blog/machine-learning-incremental-tds/</link><pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/machine-learning-incremental-tds/</guid><description/></item><item><title>Skyline queries in Python</title><link>https://maxhalford.github.io/blog/skyline-queries/</link><pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/skyline-queries/</guid><description>&lt;p>Imagine that you&amp;rsquo;re looking to buy a home. If you have an analytical mind then you might want to tackle this with a quantitative. Let&amp;rsquo;s suppose that you have a list of potential homes, and each home has some attributes that can help you compare them. As an example, we&amp;rsquo;ll consider three attributes:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>price&lt;/code> of the house, which you want to minimize&lt;/li>
&lt;li>The &lt;code>size&lt;/code> of the house, which you want to maximize&lt;/li>
&lt;li>The &lt;code>city&lt;/code> where the house if located, which you don&amp;rsquo;t really care about&lt;/li>
&lt;/ul>
&lt;p>Some houses will be objectively better than others because they will be cheaper and bigger. However, for some pairs of houses the comparison will not be as clear. It might be that house A is more expensive than house B but is also larger. In data analysis this set of best houses which are incomparable with each other is called a &lt;a href="https://www.wikiwand.com/en/Skyline_operator">skyline&lt;/a>. As they say, a picture is worth a thousand words, so let&amp;rsquo;s draw one.&lt;/p></description></item><item><title>Online machine learning with creme @ PyData Amsterdam</title><link>https://maxhalford.github.io/blog/online-machine-learning-with-creme-pydata/</link><pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-with-creme-pydata/</guid><description/></item><item><title>SQL subquery enumeration</title><link>https://maxhalford.github.io/blog/sql-subquery-enumeration/</link><pubDate>Mon, 06 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sql-subquery-enumeration/</guid><description>&lt;p>I recently stumbled on a rather fun problem during my PhD. I wanted to generate all possible subqueries from a given SQL query. In this case an example is easily worth a 1000 thousand words. Take the following SQL query:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">customers&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">purchases&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">shops&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AS&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">WHERE&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">customer_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">p&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">shop_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">nationality&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;Swedish&amp;#39;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">c&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">hair&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;Blond&amp;#39;&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="n">s&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="n">city&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;Stockholm&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here all the possible subqueries that can be generated from the above query.&lt;/p></description></item><item><title>An approach based on Bayesian networks for query selectivity estimation @ DASFAA</title><link>https://maxhalford.github.io/blog/an-approach-based-on-bayesian-networks-for-query-selectivity-estimation-slides/</link><pubDate>Mon, 22 Apr 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/an-approach-based-on-bayesian-networks-for-query-selectivity-estimation-slides/</guid><description/></item><item><title>Morellet crosses with JavaScript</title><link>https://maxhalford.github.io/blog/morellet/</link><pubDate>Sun, 03 Feb 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/morellet/</guid><description>&lt;p>The days I&amp;rsquo;m working on a deep learning project. I hate it but I promised myself to give it a real try. My scripts are taking a long time so I decided to do some procedural art while I waited. This time I&amp;rsquo;m going to reproduce the following crosses made by &lt;a href="https://www.wikiwand.com/en/Fran%C3%A7ois_Morellet">FranÃ§ois Morellet&lt;/a>. I saw them the last I went to the MusÃ©e Pompidou with some friends from university. I don&amp;rsquo;t have any smartphone anymore so one my friends was kind enough to take a few pictures for me, including this one. The painting is called &lt;a href="https://www.centrepompidou.fr/cpv/resource/cxx585o/ryjG5EL">&lt;em>Violet, bleu, vert, jaune, orange, rouge&lt;/em>&lt;/a>.&lt;/p></description></item><item><title>Streaming groupbys in pandas for big datasets</title><link>https://maxhalford.github.io/blog/pandas-streaming-groupby/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/pandas-streaming-groupby/</guid><description>&lt;p>If you&amp;rsquo;ve done a bit of Kaggling, then you&amp;rsquo;ve probably been typing a fair share of &lt;code>df.groupby(some_col)&lt;/code>. That is, if you&amp;rsquo;re using Python. If you&amp;rsquo;re handling tabular data, then a lot of your features will revolve around computing &lt;em>aggregate statistics&lt;/em>. This is very true for the ongoing &lt;a href="https://www.kaggle.com/c/PLAsTiCC-2018">PLAsTiCC Astronomical Classification challenge&lt;/a>. The goal of the competition is to classify objects in the sky into one of 14 groups. The bulk of the available data is a set of so-called &lt;em>light curve&lt;/em>. A light curve is a sequence of brightness measures observations along time. Each light curve is filtered at different passbands. The idea is that there is one light curve per passband and per object and that the shape of each light curve should tell us what kind of object we&amp;rsquo;re looking at. Yada yada.&lt;/p></description></item><item><title>Target encoding done the right way</title><link>https://maxhalford.github.io/blog/target-encoding/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/target-encoding/</guid><description>&lt;p>When you&amp;rsquo;re doing supervised learning, you often have to deal with categorical variables. That is, variables which don&amp;rsquo;t have a natural numerical representation. The problem is that most machine learning algorithms require the input data to be numerical. At some point or another a data science pipeline will require converting categorical variables to numerical variables.&lt;/p>
&lt;p>There are many ways to do so:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">Label encoding&lt;/a> where you choose an arbitrary number for each category&lt;/li>
&lt;li>&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html">One-hot encoding&lt;/a> where you create one binary column per category&lt;/li>
&lt;li>&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec">Vector representation&lt;/a> a.k.a. word2vec where you find a low dimensional subspace that fits your data&lt;/li>
&lt;li>&lt;a href="https://github.com/Microsoft/LightGBM/blob/master/docs/Advanced-Topics.rst#categorical-feature-support">Optimal binning&lt;/a> where you rely on tree-learners such as LightGBM or CatBoost&lt;/li>
&lt;li>&lt;a href="http://www.saedsayad.com/encoding.htm">Target encoding&lt;/a> where you average the target value by category&lt;/li>
&lt;/ul>
&lt;p>Each and every one of these method has its own pros and cons. The best approach typically depends on your data and your requirements. If a variable has a lot of categories, then a one-hot encoding scheme will produce many columns, which can cause memory issues. In my experience, relying on LightGBM/CatBoost is the best out-of-the-box method. Label encoding is useless and you should never use it. However if your categorical variable happens to be ordinal then you can and should represent it with increasing numbers (for example &amp;ldquo;cold&amp;rdquo; becomes 0, &amp;ldquo;mild&amp;rdquo; becomes 1, and &amp;ldquo;hot&amp;rdquo; becomes 2). &lt;a href="https://www.wikiwand.com/en/Word2vec">Word2vec&lt;/a> and others such methods are cool and good but they require some fine-tuning and don&amp;rsquo;t always work out of the box.&lt;/p></description></item><item><title>Stella triangles with JavaScript</title><link>https://maxhalford.github.io/blog/stella-triangles/</link><pubDate>Thu, 26 Apr 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/stella-triangles/</guid><description>&lt;p>Around the same time last year I visited the &lt;a href="https://www.sfmoma.org/">San Francisco Museum of Modern Art&lt;/a>. &lt;a href="https://www.wikiwand.com/en/Frank_Stella">Frank Stella&lt;/a>&amp;rsquo;s compositions really caught my eye. When I saw them I started thinking about how I could write a computer program to imitate his work. In this post I&amp;rsquo;m going to attempt to reproduce his so-called &lt;em>V Series&lt;/em>.&lt;/p>
&lt;p>&lt;img src="https://maxhalford.github.io/img/blog/stella-triangles/1.jpg" alt="1">&lt;/p>
&lt;p>&lt;img src="https://maxhalford.github.io/img/blog/stella-triangles/2.jpg" alt="2">&lt;/p>
&lt;p>Nice and simple right? Indeed in a lot of his work Frank Stella uses straight lines without much randomness. There are quite a few prints in the V Series. However in each one of them the common denominator is a single triangle. If we have a routine for drawing one triangle then we can use to make compositions. As always let&amp;rsquo;s start by creating a canvas.&lt;/p></description></item><item><title>Unknown pleasures with JavaScript</title><link>https://maxhalford.github.io/blog/unknown-pleasures/</link><pubDate>Mon, 24 Jul 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/unknown-pleasures/</guid><description>&lt;p>No this blog post is not about how nice JavaScript can be, instead it&amp;rsquo;s just another one of my attempts at reproducing modern art with &lt;a href="https://www.wikiwand.com/en/Procedural_generation">procedural generation&lt;/a> and the &lt;a href="https://www.w3schools.com/html/html5_canvas.asp">HTML5 &lt;code>&amp;lt;canvas&amp;gt;&lt;/code> element&lt;/a>. This time I randomly generated images resembling the cover of the album by Joy Division called &amp;ldquo;Unknown Pleasures&amp;rdquo;.&lt;/p>
&lt;p>&lt;img src="https://maxhalford.github.io/img/blog/unknown-pleasures/album.png" alt="album">&lt;/p>
&lt;p>&lt;a href="https://www.wikiwand.com/en/Unknown_Pleasures#/Artwork_and_packaging">According to Wikipedia&lt;/a>, this somewhat iconic album cover is based on radio waves. I saw a poster of it in a bar not long ago and decided to reproduce the next time I had some time to kill.&lt;/p></description></item><item><title>Subsampling a training set to match a test set - Part 1</title><link>https://maxhalford.github.io/blog/subsampling-1/</link><pubDate>Mon, 19 Jun 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/subsampling-1/</guid><description>&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>it&amp;rsquo;s 2022 and I still haven&amp;rsquo;t written a part 2. That&amp;rsquo;s because I believe this problem is easily solved with &lt;a href="https://www.kaggle.com/carlmcbrideellis/what-is-adversarial-validation">adversarial validation&lt;/a>&lt;/em>.&lt;/p>
&lt;p>Some friends and I recently qualified for the final of the 2017 edition of the &lt;a href="http://www.datasciencegame.com">Data Science Game&lt;/a> competition. The first part was a Kaggle competition with data provided by Deezer. The problem was a binary classification task where one had to predict if a user was going to listen to a song that was proposed to him. Like many teams we extracted clever features and trained an XGBoost classifier, classic. However, the one special thing we did was to subsample our training set so that it was more representative of the test set.&lt;/p></description></item><item><title>Docker for data science @ HelloFresh Berlin</title><link>https://maxhalford.github.io/blog/docker-for-data-science/</link><pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/docker-for-data-science/</guid><description/></item><item><title>Halftoning with Go - Part 2</title><link>https://maxhalford.github.io/blog/halftoning-2/</link><pubDate>Mon, 20 Mar 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/halftoning-2/</guid><description>&lt;p>The next stop on my travel through the world of halftoning will be the implementation of &lt;em>Weighted Voronoi Stippling&lt;/em> as described in &lt;a href="https://cs.nyu.edu/~ajsecord/">Adrian Secord&lt;/a>&amp;rsquo;s 2002 &lt;a href="http://www.mrl.nyu.edu/~ajsecord/npar2002/npar2002_ajsecord_preprint.pdf">paper&lt;/a>. This method is more involved than the ones I detailed in my &lt;a href="https://maxhalford.github.io/blog/halftoning-1">previous blog post&lt;/a>, however the results are quite interesting. Again, I did the implementation in Go.&lt;/p>
&lt;div align="center" >
&lt;figure style="width: 80%;">
 &lt;img src="https://maxhalford.github.io/img/blog/halftoning-2/coliseum.jpg" alt="colosseum">
 &lt;img src="https://maxhalford.github.io/img/blog/halftoning-2/coliseum_stippled.jpg" alt="colosseum_stippled">
 &lt;figcaption>Notice the black dot in the middle of the white square?&lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>I found a fair amount of resources about the method, most of them being implementations of Adrian Secord&amp;rsquo;s paper. However, not many of these resources went into the nitty-gritty details which are not obvious for beginners in image processing. Before delving into the code, I want to go through some concepts that may seem obvious to some readers but that I judge worthy of detailing.&lt;/p></description></item><item><title>Grid paintings Ã  la Mondrian with JavaScript</title><link>https://maxhalford.github.io/blog/mondrian/</link><pubDate>Sat, 04 Mar 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/mondrian/</guid><description>&lt;p>I was at a laundrette today and had just finished my book so I had some time to kill. Naturally I devised an algorithm for generating drawings that would resemble the &lt;a href="https://www.google.co.uk/search?q=piet+mondrian+grid+painting">grid-like paintings&lt;/a> that &lt;a href="https://en.wikipedia.org/wiki/Piet_Mondrian">Piet Mondrian&lt;/a> made famous. With the benefit of hindsight I guess I could indulge in saner activities while waiting for my laundry to dry!&lt;/p>
&lt;p>I went through different ideas but in the end I settled on a recursive approach. My idea is to divide a rectangle into two smaller ones and then to do the same with each sub-rectangle. Every time a rectangle is generated and is then filled with a random color; like Mondrian I use yellow, red, blue, black and white.&lt;/p></description></item><item><title>A short introduction and conclusion to the OpenBikes 2016 Challenge</title><link>https://maxhalford.github.io/blog/openbikes-challenge/</link><pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/openbikes-challenge/</guid><description>&lt;p>During my undergraduate internship in 2015 I started a side project called OpenBikes. The idea was to visualize and analyze bike sharing over multiple cities. &lt;a href="http://axelbellec.fr/">Axel Bellec&lt;/a> joined me and in 2016 we &lt;a href="http://www.opendatafrance.net/2016/02/05/le-prix-open-data-toulouse-metropole-remis-a-openbikes">won a national open data competition&lt;/a>. Since then we haven&amp;rsquo;t pursued anything major, instead we use OpenBikes to try out technologies and to apply concepts we learn at university and online.&lt;/p>
&lt;p>Before the 2016 summer holidays one of my professors, &lt;a href="https://www.math.univ-toulouse.fr/~agarivie/">AurÃ©lien Garivier&lt;/a> mentioned that he was considering using our data for a Kaggle-like competition between some statistics curriculums in France. Near the end of the summer, I sat down with a group of professors and we decided upon a format for the so-called &amp;ldquo;Challenge&amp;rdquo;. The general idea was to provide student teams with historical data on multiple bike stations and ask them to do some forecasting which we would then score based on a secret truth. The whole thing lasted from the 5th of October 2016 till the 26th of January 2017 when the best team was crowned.&lt;/p></description></item><item><title>Challenge Big Data @ TSE</title><link>https://maxhalford.github.io/blog/challenge-big-data/</link><pubDate>Mon, 09 Jan 2017 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/challenge-big-data/</guid><description/></item><item><title>Halftoning with Go - Part 1</title><link>https://maxhalford.github.io/blog/halftoning-1/</link><pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/halftoning-1/</guid><description>&lt;p>Recently I stumbled upon &lt;a href="http://www.cgl.uwaterloo.ca/csk/projects/tsp/">this webpage&lt;/a> which shows how to use a TSP solver as a &lt;a href="https://www.wikiwand.com/en/Halftone">&lt;em>halftoning&lt;/em>&lt;/a> technique. I began to read about related concepts like &lt;a href="https://www.wikiwand.com/en/Dither">&lt;em>dithering&lt;/em>&lt;/a> and &lt;a href="https://www.wikiwand.com/en/Stippling">&lt;em>stippling&lt;/em>&lt;/a>. I don&amp;rsquo;t have any background in photography but I can appreciate the visual appeal of these techniques. As I understand it these techniques were first invented to save ink for printing. However nowadays printing has become cheaper and the modern use of these technique is mostly aesthetic, at least for images.&lt;/p></description></item><item><title>Predire la disponibilitÃ© des Velib' @ Toulouse Data Science Meetup</title><link>https://maxhalford.github.io/blog/forecasting-bicycle-sharing-usage/</link><pubDate>Wed, 30 Mar 2016 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/forecasting-bicycle-sharing-usage/</guid><description/></item><item><title>Recursive polygons with JavaScript</title><link>https://maxhalford.github.io/blog/recursive-polygons/</link><pubDate>Fri, 25 Mar 2016 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/recursive-polygons/</guid><description>&lt;p>I like modern art, I enjoy looking at the stuff that was made at the beginning of the 20th century and thinking how it is still shaping today&amp;rsquo;s style. I&amp;rsquo;m not an expert, it&amp;rsquo;s just a hobby of mine. I especially like the &lt;a href="https://www.centrepompidou.fr/">Centre Pompidou&lt;/a> in Paris, it&amp;rsquo;s got loads of fascinating stuff. While I was going through the galleries it struck me that some of the paintings were very geometrical. In fact they were so geometrical that a machine could have produced them! I&amp;rsquo;m not talking about artificial intelligence but rather a set of rules that could be given to a programming language. Through a series of blog posts I would like to try to emulate some works with my computer. I realize it&amp;rsquo;s a waste of time but it&amp;rsquo;s a good opportunity for me to learn some more JavaScript and refreshen my geometry. I also want to insist on making these drawings random, not deterministic.&lt;/p></description></item><item><title>The NaÃ¯ve Bayes classifier</title><link>https://maxhalford.github.io/blog/naive-bayes/</link><pubDate>Thu, 10 Sep 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/naive-bayes/</guid><description>&lt;p>The objective of a classifier is to decide to which &lt;em>class&lt;/em> (also called &lt;em>label&lt;/em>) to assign an observation based on observed data. In &lt;em>supervised learning&lt;/em>, this is done by taking into account previous classifications. In other words if we &lt;em>know&lt;/em> that certain observations are classified in a certain way, the goal is to determine the class of a new observation. The first group of observations on which the classifier is built is called the &lt;em>training set&lt;/em>.&lt;/p></description></item><item><title>An introduction to genetic algorithms</title><link>https://maxhalford.github.io/blog/genetic-algorithms-introduction/</link><pubDate>Sun, 02 Aug 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/genetic-algorithms-introduction/</guid><description>&lt;p>The goal of genetic algorithms (GAs) is to solve problems whose solutions are not easily found (ie. NP problems, nonlinear optimization, etc.). For example, finding the shortest path from A to B in a directed graph is easily done with &lt;em>Djikstra&amp;rsquo;s algorithm&lt;/em>, it can be solved in polynomial time. However the time to find the smallest path that joins all points on a non-directed graph, also known as the &lt;a href="http://www.wikiwand.com/en/Travelling_salesman_problem">Travelling Salesman Problem&lt;/a> (TSP) increases exponentially as the number of points increases. More generally, GAs are useful for problems where an analytical approach is complicated or even impossible. By giving up on perfection they manage to find a good approximation of the optimal solution.&lt;/p></description></item><item><title>Setting up a droplet to host a Flask app</title><link>https://maxhalford.github.io/blog/flask-droplet/</link><pubDate>Tue, 14 Jul 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/flask-droplet/</guid><description>&lt;p>After having worked for some weeks on the &lt;a href="http://openbikes.co">OpenBikes website&lt;/a>, it was time to put it online. &lt;a href="https://www.digitalocean.com/">Digital Ocean&lt;/a> seemed to provide a good service and so I decided to give it a spin. Their documentation is quite good but it doesn&amp;rsquo;t cover exactly everything for setting up Flask. In this post I simply want to record every single step I took.&lt;/p>
&lt;p>OpenBikes is a project with a Flask backend and a few upstart jobs. It lives at the &lt;a href="http://openbikes.co">openbikes.co&lt;/a> domain name. In this blog post I will list every step it takes to make it happen on Ubuntu 14.04 with Apache (it&amp;rsquo;s robust and easy to setup). I didn&amp;rsquo;t always say when to use &lt;code>sudo&lt;/code> before the commands to avoid clutter, however you can safely use it everywhere.&lt;/p></description></item><item><title>Visualizing bike stations live data</title><link>https://maxhalford.github.io/blog/bike-stations/</link><pubDate>Wed, 03 Jun 2015 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/bike-stations/</guid><description>&lt;p>Recently some friends and I decided to launch &lt;a href="http://openbikes.co/">openbikes.co&lt;/a>, a website for visualizing (and later on analyzing) urban bike traffic. We have a lot of ideas that we will progressively implement. Anyway, the point is that all of it started with me fiddling about with the &lt;em>JCDecaux&lt;/em> API and the &lt;em>leaflet.js&lt;/em> library and I would like to share it with you. Shall we?&lt;/p>
&lt;h2 id="presentation">Presentation&lt;/h2>
&lt;p>In this post I want to show you the tools and the code to get a fully functional website for visualizing live data. In this particular case we will display bike stations in Toulouse, however I will keep the scripts as general as possible so they are easily modifiable for different data. Before starting here is a glimpse of the end result:&lt;/p></description></item></channel></rss>