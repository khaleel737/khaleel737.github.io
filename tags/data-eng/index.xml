<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data-Eng on Max Halford</title><link>https://maxhalford.github.io/tags/data-eng/</link><description>Recent content in Data-Eng on Max Halford</description><generator>Hugo</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Mon, 09 Jun 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/tags/data-eng/index.xml" rel="self" type="application/rss+xml"/><item><title>Thoughts on DuckLake</title><link>https://maxhalford.github.io/blog/ducklake-thoughts/</link><pubDate>Mon, 09 Jun 2025 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/ducklake-thoughts/</guid><description>&lt;p>&lt;a href="https://ducklake.select/">DuckLake&lt;/a> is the new data lake/warehouse from the makers of DuckDB. I really like the direction they&amp;rsquo;re taking. I&amp;rsquo;m hopeful it has the potential to streamline the data engineering workflow for many people, vastly reducing costs along the way.&lt;/p>
&lt;p>I&amp;rsquo;m a bit of a nut and don&amp;rsquo;t use SQLMesh or dbt. Instead, I built &lt;a href="https://github.com/carbonfact/lea">lea&lt;/a> a few years ago, and we still use it at &lt;a href="https://carbonfact.org">Carbonfact&lt;/a>. I would probably pick SQLMesh if I had to start over, but lea allows me to explore new ideas, so I&amp;rsquo;m sticking to it for now.&lt;/p></description></item><item><title>A training set for bike sharing forecasting</title><link>https://maxhalford.github.io/blog/bike-sharing-forecasting-training-set/</link><pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/bike-sharing-forecasting-training-set/</guid><description>&lt;style>
table {
 font-family: monospace; /* Apply monospace font */
}

table td, table th {
 white-space: nowrap; /* Prevent text from wrapping */
}
&lt;/style>
&lt;p>Last night I went to a &lt;a href="https://www.meetup.com/fr-FR/tlse-data-science/">Toulouse Data Science&lt;/a> meetup. The talks were about generative AI and information retrieval, which aren&amp;rsquo;t topics I&amp;rsquo;m knowledgeable about. However, one of the speakers was a &lt;a href="https://github.com/raphaelsty">friend&lt;/a> of mine, so I went to support him. Toulouse is my hometown, so I bumped into a few people I knew. It was a nice evening.&lt;/p></description></item><item><title>Efficient ELT refreshes</title><link>https://maxhalford.github.io/blog/efficient-data-transformation/</link><pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/efficient-data-transformation/</guid><description>&lt;p>A tenant of the modern data stack is the use of ELT (Extract, Load, Transform) over ETL (Extract, Transform, Load). In a nutshell, this means that most of the data transformation is done in the data warehouse. This has become the &lt;em>de facto&lt;/em> standard for modern data teams, and is epitomized by &lt;a href="https://www.getdbt.com/">dbt&lt;/a> and its ecosystem. It&amp;rsquo;s a great time to be a data engineer!&lt;/p>
&lt;p>We at &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a> fully embrace the ELT paradigm. In fact, our whole platform is powered by BigQuery, which acts as our single source of truth. We have a main BigQuery dataset where we materialize severalÂ SQL views that power what our customers see.&lt;/p></description></item><item><title>Sh*t flows downhill, but not at Carbonfact</title><link>https://maxhalford.github.io/blog/shit-flows-downhill-but-not-at-carbonfact/</link><pubDate>Mon, 16 Oct 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/shit-flows-downhill-but-not-at-carbonfact/</guid><description>&lt;p>I&amp;rsquo;m writing this after watching the talk &lt;a href="https://josephreis.com/">Joe Reis&lt;/a> gave at &lt;a href="https://bigdataldn.com/">Big Data LDN&lt;/a>. It&amp;rsquo;s called &lt;a href="https://www.youtube.com/watch?app=desktop&amp;amp;v=OCClTPOEe5s&amp;amp;ref=blef.fr">Data Modeling is Dead! Long Live Data Modeling!&lt;/a> It&amp;rsquo;s an easy-to-watch short talk that calls out on a few modern issues in the data world.&lt;/p>
&lt;p>I&amp;rsquo;d like to bounce off one of Joe&amp;rsquo;s slides:&lt;/p>
&lt;div align="center" >
&lt;figure style="width: 80%; margin: 0;">
 &lt;img src="https://maxhalford.github.io/img/blog/shit-flows-downhill-but-not-at-carbonfact/shit-flows-downhill.png">
&lt;/figure>
&lt;/div>
&lt;p>I&amp;rsquo;m aligned with Joe that many issues stem from the lack of unison between engineering and data teams. A fundamental aspect of the Modern Data Stack is to replicate/copy production data into an analytics warehouse. For instance, copying the production PostgreSQL database into BigQuery.&lt;/p></description></item><item><title>For analytics, don't use dynamic JSON keys</title><link>https://maxhalford.github.io/blog/no-dynamic-keys-in-json/</link><pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/no-dynamic-keys-in-json/</guid><description>&lt;p>I love the JSON format. It&amp;rsquo;s the kind of love that grows on you with time. Like others, I&amp;rsquo;ve been using JSON everywhere for so many years, to the point where I just take it for granted.&lt;/p>
&lt;p>I suppose the main thing I like about JSON is its flexibility. You can structure your JSONs without too much care. There will always be a way to consume and manipulate it. But I have discovered a bit of anti-pattern, which I believe is worth raising awareness about. Especially when you&amp;rsquo;re doing analytics.&lt;/p></description></item><item><title>A rant against dbt ref</title><link>https://maxhalford.github.io/blog/dbt-ref-rant/</link><pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/dbt-ref-rant/</guid><description>&lt;h2 id="disclaimer">Disclaimer&lt;/h2>
&lt;p>&lt;em>Let me be absolutely clear: I think dbt is a great tool. Although this post is a rant, the goal is to be constructive and suggest an improvement.&lt;/em>&lt;/p>
&lt;h2 id="dbt-in-a-nutshell">dbt in a nutshell&lt;/h2>
&lt;p>&lt;a href="https://www.getdbt.com/">dbt&lt;/a> is a workflow orchestrator for SQL. In other words, it&amp;rsquo;s a fancy &lt;a href="https://www.wikiwand.com/en/Make_(software)">Make&lt;/a> for data analytics. What makes dbt special is that it is the first workflow orchestrator that is dedicated to the SQL language. It said out loud what many data teams were thinking: you can get a lot done with SQL.&lt;/p></description></item><item><title>Dashboards and GROUPING SETS</title><link>https://maxhalford.github.io/blog/grouping-sets/</link><pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/grouping-sets/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>At &lt;a href="https://alan.com/">Alan&lt;/a>, we do almost all our data analysis in SQL. Our data warehouse used to be &lt;a href="https://www.postgresql.org/">PostgreSQL&lt;/a>, and have since switched to &lt;a href="https://www.snowflake.com/">Snowflake&lt;/a> for performance reasons. We load data into our warehouse with &lt;a href="https://airflow.apache.org/">Airflow&lt;/a>. This includes dumps of our production database, third-party data, and health data from other actors in the health ecosystem. This is raw data. We transform this into prepared data via an in-house tool that resembles &lt;a href="https://www.getdbt.com/">dbt&lt;/a>. You can read more about it &lt;a href="https://medium.com/alan/how-we-solve-the-problem-of-sharing-actionable-data-with-the-team-7e4afeff3cac">here&lt;/a>.&lt;/p></description></item><item><title>An overview of dataset time travel</title><link>https://maxhalford.github.io/blog/dataset-time-travel/</link><pubDate>Wed, 07 Apr 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/dataset-time-travel/</guid><description>&lt;h2 id="tldr">TLDR&lt;/h2>
&lt;p>You&amp;rsquo;re a data scientist. The engineers in your company overwrite data in the production database. You want to access overwritten data to train your models. How?&lt;/p>
&lt;h2 id="i-thought-time-travel-only-existed-in-the-movies">I thought time travel only existed in the movies&lt;/h2>
&lt;p>You&amp;rsquo;re probably right, expect maybe for &lt;a href="https://www.wikiwand.com/en/Time_travel_claims_and_urban_legends#/Present-day_hipster_at_1941_bridge_opening">this guy&lt;/a>.&lt;/p>
&lt;p>I want to discuss a concept that&amp;rsquo;s been on my mind for a while now. I like to call it &amp;ldquo;dataset time travel&amp;rdquo; because it has a nice ring to it. But the association of &amp;ldquo;time travel&amp;rdquo; and &amp;ldquo;data&amp;rdquo; has already been used elsewhere. It&amp;rsquo;s not something I&amp;rsquo;m pulling out from thin air. Essentially, what I want to discuss is the ability to view a dataset at any given point in the past. Having this ability is powerful, as it allows answering important business questions. As an example, let&amp;rsquo;s say we have a database table called &lt;code>users&lt;/code>. We might ask the following question:&lt;/p></description></item><item><title>A few intermediate pandas tricks</title><link>https://maxhalford.github.io/blog/pandas-tricks/</link><pubDate>Mon, 17 Aug 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/pandas-tricks/</guid><description>&lt;p>I want to use this post to share some &lt;code>pandas&lt;/code> snippets that I find useful. I use them from time to time, in particular when I&amp;rsquo;m doing &lt;a href="https://www.kaggle.com/search?q=time+series+in%3Acompetitions">time series competitions&lt;/a> on platforms such as Kaggle. Like any data scientist, I perform similar data processing steps on different datasets. Usually, I put repetitive patterns in &lt;a href="https://github.com/MaxHalford/xam">&lt;code>xam&lt;/code>&lt;/a>, which is my personal data science toolbox. However, I think that the following snippets are too small and too specific for being added into a library.&lt;/p></description></item><item><title>Finding fuzzy duplicates with pandas</title><link>https://maxhalford.github.io/blog/transitive-duplicates/</link><pubDate>Mon, 16 Sep 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/transitive-duplicates/</guid><description>&lt;p>Duplicate detection is the task of finding two or more instances in a dataset that are in fact identical. As an example, take the following toy dataset:&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th style="text-align: center">&lt;/th>
 &lt;th style="text-align: center">&lt;strong>First name&lt;/strong>&lt;/th>
 &lt;th style="text-align: center">&lt;strong>Last name&lt;/strong>&lt;/th>
 &lt;th style="text-align: center">&lt;strong>Email&lt;/strong>&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td style="text-align: center">0&lt;/td>
 &lt;td style="text-align: center">Erlich&lt;/td>
 &lt;td style="text-align: center">Bachman&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.com">eb@piedpiper.com&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">1&lt;/td>
 &lt;td style="text-align: center">Erlich&lt;/td>
 &lt;td style="text-align: center">Bachmann&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.com">eb@piedpiper.com&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">2&lt;/td>
 &lt;td style="text-align: center">Erlik&lt;/td>
 &lt;td style="text-align: center">Bachman&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.co">eb@piedpiper.co&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td style="text-align: center">3&lt;/td>
 &lt;td style="text-align: center">Erlich&lt;/td>
 &lt;td style="text-align: center">Bachmann&lt;/td>
 &lt;td style="text-align: center">&lt;a href="mailto:eb@piedpiper.com">eb@piedpiper.com&lt;/a>&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>Each of these instances (rows, if you prefer) corresponds to the same &amp;ldquo;thing&amp;rdquo; &amp;ndash; note that I&amp;rsquo;m not using the word &amp;ldquo;entity&amp;rdquo; because &lt;a href="https://www.wikiwand.com/en/Record_linkage#/Entity_resolution">entity resolution&lt;/a> is a different, and yet related, concept. In my experience there are two main reasons why data duplication may occur:&lt;/p></description></item><item><title>A smooth approach to putting machine learning into production</title><link>https://maxhalford.github.io/blog/machine-learning-production/</link><pubDate>Sat, 13 Jul 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/machine-learning-production/</guid><description>&lt;p>Putting machine learning into production is hard. Usually I&amp;rsquo;m doubtful of such statements, but in this case I&amp;rsquo;ve never met anyone for whom everything has gone smoothly. Most data scientists might agree that there is a huge gap between their local environment and a live environment. In fact, &amp;ldquo;productionalizing&amp;rdquo; machine learning is such a complex topic that entire companies have risen to address the issue. I&amp;rsquo;m not just talking about running a gigantic grid search and finding the best model, I&amp;rsquo;m talking about putting a machine learning model live so that it actually has a positive impact on your business/project. Off the top of my head: &lt;a href="https://www.cubonacci.com/">Cubonacci&lt;/a>, &lt;a href="https://www.h2o.ai/">H2O&lt;/a>, &lt;a href="https://cloud.google.com/automl/">Google AutoML&lt;/a>, &lt;a href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html">Amazon Sagemaker&lt;/a>, and &lt;a href="https://www.datarobot.com/">DataRobot&lt;/a>. In other words people are making money off businesses because data scientists and engineers are having a hard putting their models into production. In my opinion if a data scientist can&amp;rsquo;t put her model into production herself then something is wrong. Life should be simpler.&lt;/p></description></item><item><title>Skyline queries in Python</title><link>https://maxhalford.github.io/blog/skyline-queries/</link><pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/skyline-queries/</guid><description>&lt;p>Imagine that you&amp;rsquo;re looking to buy a home. If you have an analytical mind then you might want to tackle this with a quantitative. Let&amp;rsquo;s suppose that you have a list of potential homes, and each home has some attributes that can help you compare them. As an example, we&amp;rsquo;ll consider three attributes:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>price&lt;/code> of the house, which you want to minimize&lt;/li>
&lt;li>The &lt;code>size&lt;/code> of the house, which you want to maximize&lt;/li>
&lt;li>The &lt;code>city&lt;/code> where the house if located, which you don&amp;rsquo;t really care about&lt;/li>
&lt;/ul>
&lt;p>Some houses will be objectively better than others because they will be cheaper and bigger. However, for some pairs of houses the comparison will not be as clear. It might be that house A is more expensive than house B but is also larger. In data analysis this set of best houses which are incomparable with each other is called a &lt;a href="https://www.wikiwand.com/en/Skyline_operator">skyline&lt;/a>. As they say, a picture is worth a thousand words, so let&amp;rsquo;s draw one.&lt;/p></description></item></channel></rss>