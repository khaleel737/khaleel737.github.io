<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Online-Machine-Learning on Max Halford</title><link>https://maxhalford.github.io/tags/online-machine-learning/</link><description>Recent content in Online-Machine-Learning on Max Halford</description><generator>Hugo</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Thu, 26 Oct 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/tags/online-machine-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Online machine learning on the road @ IDE+A, TH K√∂ln</title><link>https://maxhalford.github.io/blog/online-machine-learning-on-the-road/</link><pubDate>Thu, 26 Oct 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-on-the-road/</guid><description/></item><item><title>Online gradient descent written in SQL</title><link>https://maxhalford.github.io/blog/ogd-in-sql/</link><pubDate>Tue, 07 Mar 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/ogd-in-sql/</guid><description>&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>this post &lt;a href="https://news.ycombinator.com/item?id=35054786">generated&lt;/a> a few insightful comments on Hacker News. I&amp;rsquo;ve also put the code in a &lt;a href="https://gist.github.com/MaxHalford/823c4e7f9216607dc853724ec74ec692">notebook&lt;/a> for ease of use.&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Modern MLOps is complex because it involves too many components. You need a message bus, a stream processing engine, an API, a model store, a feature store, a monitoring service, etc. Sadly, containerisation software and the unbundling trend have encouraged an appetite for complexity. I believe MLOps shouldn&amp;rsquo;t be this complex. For instance, MLOps can be made simpler by &lt;a href="https://www.ethanrosenthal.com/2022/05/10/database-bundling/">bundling the logic into your database&lt;/a>.&lt;/p></description></item><item><title>Online active learning in 80 lines of Python</title><link>https://maxhalford.github.io/blog/online-active-learning-river-databutton/</link><pubDate>Sun, 22 Jan 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-active-learning-river-databutton/</guid><description>&lt;p>&lt;a href="https://www.wikiwand.com/en/Active_learning_(machine_learning)">Active learning&lt;/a> is a way to get humans to label data efficiently. A good active learning strategy minimizes the number of necessary labels, while maximizing a model&amp;rsquo;s performance. This usually works by focusing on samples where the model is unsure of its prediction.&lt;/p>
&lt;p>In a batch setting, the model is periodically retrained to learn from the freshly labeled samples. However, the training time is usually too prohibitive for this to happen each time a new label is provided. This isn&amp;rsquo;t the case with online models, because they are able to learn one sample at a time. Active and online learning naturally fit together.&lt;/p></description></item><item><title>The future of River</title><link>https://maxhalford.github.io/blog/future-of-river/</link><pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/future-of-river/</guid><description>&lt;div align="center">
&lt;figure >
 &lt;img src="https://maxhalford.github.io/img/blog/future-of-river/tweet.png" style="box-shadow: none;">
 &lt;figcaption>&lt;a href="https://twitter.com/josh_wills/status/1585328751646109696">Source&lt;/a>&lt;/figcaption>
&lt;/figure>
&lt;/div>
&lt;p>When I see tweets like this one, I&amp;rsquo;m both happy because people are aware of &lt;a href="https://riverml.xyz/">River&lt;/a>, but also irked because it&amp;rsquo;s really difficult to make production-grade open source software.&lt;/p>
&lt;p>We just had a developer meeting a week ago. We planned &lt;a href="https://github.com/orgs/online-ml/projects/3?query=is%3Aopen+sort%3Aupdated-desc">what we will work on&lt;/a> during the first half of 2023. I thought it would be worthwhile to give a high-level view of how we envision River&amp;rsquo;s future. If not to be comprehensive, at least to reassure potential users that River is alive and kicking ü§∫&lt;/p></description></item><item><title>Matrix inverse mini-batch updates</title><link>https://maxhalford.github.io/blog/matrix-inverse-mini-batch/</link><pubDate>Wed, 24 Aug 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/matrix-inverse-mini-batch/</guid><description>&lt;p>The inverse covariance matrix, also called &lt;a href="https://www.wikiwand.com/en/Precision_matrix">precision matrix&lt;/a>, is useful in many places across the field of statistics. For instance, in machine learning, it is used for &lt;a href="https://maxhalford.github.io/blog/bayesian-linear-regression">Bayesian regression&lt;/a> and &lt;a href="https://scikit-learn.org/stable/modules/mixture.html#gmm">mixture modelling&lt;/a>.&lt;/p>
&lt;p>What&amp;rsquo;s interesting is that any batch model which uses a precision matrix can be turned into an online model. That is, provided the precision matrix can be estimated in a streaming fashion. For instance, scikit-learn&amp;rsquo;s &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html#sklearn.covariance.EllipticEnvelope">elliptic envelope&lt;/a> method could have an online variant with a &lt;code>partial_fit&lt;/code> method.&lt;/p></description></item><item><title>First IRL meetup with the River developers</title><link>https://maxhalford.github.io/blog/first-river-meetup/</link><pubDate>Thu, 09 Jun 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/first-river-meetup/</guid><description>&lt;p>&lt;a href="https://github.com/online-ml/river/">River&lt;/a> is a Python software for doing online machine learning. It&amp;rsquo;s the result of a merger in early 2020 between &lt;a href="https://github.com/online-ml/river">creme&lt;/a> and &lt;a href="https://github.com/scikit-multiflow/scikit-multiflow">scikit-multiflow&lt;/a>. &lt;a href="https://smastelini.github.io/">Saulo Mastelini&lt;/a>, &lt;a href="https://jacobmontiel.github.io/">Jacob Montiel&lt;/a>, and myself are the three core developers. But there are many more people who contribute here and there!&lt;/p>
&lt;p>This week Saulo Mastelini and I got to meet in person. This is worth mentioning because Saulo is originally from Brazil, whereas I&amp;rsquo;m based in Europe. We connected and I&amp;rsquo;m glad to think of him as a good friend from now on. Of course we were not alone: some friends of mine from university also joined the fun. These are people who initially contributed to creme, back in what we already call the old days! Each one of them has their own areas of expertise, and contributed to various parts of the codebase.&lt;/p></description></item><item><title>Online machine learning with River @ GAIA</title><link>https://maxhalford.github.io/blog/online-machine-learning-with-river/</link><pubDate>Thu, 07 Apr 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-with-river/</guid><description/></item><item><title>Online machine learning in practice @ PyData PDX</title><link>https://maxhalford.github.io/blog/online-machine-learning-in-practice-pydata-pdx/</link><pubDate>Wed, 09 Feb 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-in-practice-pydata-pdx/</guid><description/></item><item><title>The online machine learning predict/fit switcheroo</title><link>https://maxhalford.github.io/blog/predict-fit-switcheroo/</link><pubDate>Thu, 06 Jan 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/predict-fit-switcheroo/</guid><description>&lt;h2 id="why-im-writing-this">Why I&amp;rsquo;m writing this&lt;/h2>
&lt;p>Fact: designing open source software is hard. It&amp;rsquo;s difficult to make design decisions which don&amp;rsquo;t make any compromises. I like to fall back on Dieter Rams&amp;rsquo; &lt;a href="https://ifworlddesignguide.com/design-specials/dieter-rams-10-principles-for-good-design">10 principles for good design&lt;/a>. I feel like they apply rather well to software design. Especially when said software is open source, due to the many users and the plethora of use cases.&lt;/p>
&lt;p>I had to make a significant design decision for &lt;a href="https://github.com/online-ml/river/">River&lt;/a>. It boils down to the fact that making a prediction with a model pipeline is a stateful operation, whereas users understandably expect it to be pure with no side-effects. This regularly comes up on the issue tracker, as you can see &lt;a href="https://github.com/online-ml/river/issues/130">here&lt;/a>, &lt;a href="https://github.com/online-ml/river/issues/359">here&lt;/a>, and &lt;a href="https://github.com/online-ml/river/issues/499">here&lt;/a>.&lt;/p></description></item><item><title>Online machine learning in practice @ Applied AI</title><link>https://maxhalford.github.io/blog/real-time-ml-next-frontier-applied-ai/</link><pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/real-time-ml-next-frontier-applied-ai/</guid><description/></item><item><title>Online machine learning in practice @ LVMH</title><link>https://maxhalford.github.io/blog/real-time-ml-next-frontier-lvmh/</link><pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/real-time-ml-next-frontier-lvmh/</guid><description/></item><item><title>The challenges of online machine learning in production @ Ita√∫ Unibanco</title><link>https://maxhalford.github.io/blog/challenges-of-online-machine-learning-in-production/</link><pubDate>Fri, 26 Feb 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/challenges-of-online-machine-learning-in-production/</guid><description/></item><item><title>A brief introduction to online machine learning @ Hong Kong Machine Learning Meetup</title><link>https://maxhalford.github.io/blog/brief-introduction-to-online-machine-learning/</link><pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/brief-introduction-to-online-machine-learning/</guid><description/></item><item><title>The correct way to evaluate online machine learning models</title><link>https://maxhalford.github.io/blog/online-learning-evaluation/</link><pubDate>Sun, 07 Jun 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-learning-evaluation/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>Most supervised machine learning algorithms work in the batch setting, whereby they are fitted on a training set offline, and are used to predict the outcomes of new samples. The only way for batch machine learning algorithms to learn from new samples is to train them from scratch with both the old samples and the new ones. Meanwhile, some learning algorithms are online, and can predict as well as update themselves when new samples are available. This encompasses any model trained with &lt;a href="https://leon.bottou.org/publications/pdf/compstat-2010.pdf">stochastic gradient descent&lt;/a> &amp;ndash; which includes deep neural networks, &lt;a href="https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf">factorisation machines&lt;/a>, and &lt;a href="https://www.cs.huji.ac.il/~shais/papers/ShalevSiSrCo10.pdf">SVMs&lt;/a> &amp;ndash; as well as &lt;a href="https://homes.cs.washington.edu/~pedrod/papers/kdd00.pdf">decision trees&lt;/a>, &lt;a href="https://ai.stanford.edu/~ang/papers/icml04-onlinemetric.pdf">metric learning&lt;/a>, and &lt;a href="https://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf">na√Øve Bayes&lt;/a>.&lt;/p></description></item><item><title>Online machine learning with decision trees @ Toulouse AOC workgroup</title><link>https://maxhalford.github.io/blog/online-machine-learning-with-decision-trees/</link><pubDate>Thu, 07 May 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-with-decision-trees/</guid><description/></item><item><title>Machine learning for streaming data with creme</title><link>https://maxhalford.github.io/blog/medium-creme/</link><pubDate>Thu, 26 Mar 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/medium-creme/</guid><description/></item><item><title>The benefits of online machine learning @ Quantmetry</title><link>https://maxhalford.github.io/blog/the-benefits-of-online-learning-quantmetry/</link><pubDate>Tue, 29 Oct 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/the-benefits-of-online-learning-quantmetry/</guid><description/></item><item><title>The benefits of online machine learning @ Element AI</title><link>https://maxhalford.github.io/blog/the-benefits-of-online-learning-element-ai/</link><pubDate>Wed, 23 Oct 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/the-benefits-of-online-learning-element-ai/</guid><description/></item><item><title>The benefits of online machine learning @ Airbus Bizlab</title><link>https://maxhalford.github.io/blog/the-benefits-of-online-learning-airbus-bizlab/</link><pubDate>Fri, 28 Jun 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/the-benefits-of-online-learning-airbus-bizlab/</guid><description/></item><item><title>Machine learning incr√©mental: des concepts √† la pratique @ Toulouse Data Science Meetup</title><link>https://maxhalford.github.io/blog/machine-learning-incremental-tds/</link><pubDate>Tue, 28 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/machine-learning-incremental-tds/</guid><description/></item><item><title>Online machine learning with creme @ PyData Amsterdam</title><link>https://maxhalford.github.io/blog/online-machine-learning-with-creme-pydata/</link><pubDate>Sat, 11 May 2019 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/online-machine-learning-with-creme-pydata/</guid><description/></item><item><title>Streaming groupbys in pandas for big datasets</title><link>https://maxhalford.github.io/blog/pandas-streaming-groupby/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/pandas-streaming-groupby/</guid><description>&lt;p>If you&amp;rsquo;ve done a bit of Kaggling, then you&amp;rsquo;ve probably been typing a fair share of &lt;code>df.groupby(some_col)&lt;/code>. That is, if you&amp;rsquo;re using Python. If you&amp;rsquo;re handling tabular data, then a lot of your features will revolve around computing &lt;em>aggregate statistics&lt;/em>. This is very true for the ongoing &lt;a href="https://www.kaggle.com/c/PLAsTiCC-2018">PLAsTiCC Astronomical Classification challenge&lt;/a>. The goal of the competition is to classify objects in the sky into one of 14 groups. The bulk of the available data is a set of so-called &lt;em>light curve&lt;/em>. A light curve is a sequence of brightness measures observations along time. Each light curve is filtered at different passbands. The idea is that there is one light curve per passband and per object and that the shape of each light curve should tell us what kind of object we&amp;rsquo;re looking at. Yada yada.&lt;/p></description></item></channel></rss>