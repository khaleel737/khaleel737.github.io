<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on Max Halford</title><link>https://maxhalford.github.io/tags/python/</link><description>Recent content in Python on Max Halford</description><generator>Hugo</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Sat, 08 Feb 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>Minimizing the runtime of a SQL DAG</title><link>https://maxhalford.github.io/blog/minimizing-sql-dag-runtime/</link><pubDate>Sat, 08 Feb 2025 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/minimizing-sql-dag-runtime/</guid><description>&lt;p>I recently looked into reducing the runtime of &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a>&amp;rsquo;s SQL DAG. Our DAG is made up of roughly 160 SQL queries. It takes about 10 minutes to run with BigQuery, using on-demand pricing. It&amp;rsquo;s decent. However, the results of our DAG feed customer dashboards, and we have the (bad) habit of refreshing the DAG several times a day. Reducing the runtime by a few minutes can be a nice quality-of-life improvement.&lt;/p></description></item><item><title>Introducing icanexplain @ PyData Paris 2024</title><link>https://maxhalford.github.io/blog/icanexplain-pydata/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/icanexplain-pydata/</guid><description/></item><item><title>@daily_cache implementation in Python</title><link>https://maxhalford.github.io/blog/python-daily-cache/</link><pubDate>Tue, 27 Aug 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/python-daily-cache/</guid><description>&lt;p>I spend a lot of time at Carbonfact working on datasets shared by our customers. We typically set things up so that our customers can export data automatically. They usually deposit files to a GCP bucket, with a script, once a day. We then have an ETL script for each customer that runs afterwards to fetch their latest data and process it.&lt;/p>
&lt;p>During development, I load customer data to my laptop and work on it. The datasets can be quite heavy, and it takes time to fetch them, so I cache them to save some time. Python has &lt;a href="https://docs.python.org/3/library/functools.html">something&lt;/a> for this in its standard library:&lt;/p></description></item><item><title>LCA software: exit the matrix</title><link>https://maxhalford.github.io/blog/lca-exit-the-matrix/</link><pubDate>Sun, 09 Jun 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/lca-exit-the-matrix/</guid><description>&lt;p>Measuring the environmental impact of a product is done using &lt;a href="https://en.wikipedia.org/wiki/Life-cycle_assessment">life cycle assessment&lt;/a> (LCA). This is a methodology that breaks down a product&amp;rsquo;s life cycle into stages (&lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment#Life_cycle_inventory_(LCI)">LCI&lt;/a>), and measures the impact of each stage on the environment (&lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment#Life_cycle_impact_assessment_(LCIA)">LCIA&lt;/a>).&lt;/p>
&lt;p>There are a few pieces of LCA software to choose from. The leading ones are &lt;a href="https://simapro.com/">SimaPro&lt;/a>, &lt;a href="https://sphera.com/life-cycle-assessment-lca-software/">GaBi&lt;/a>, &lt;a href="https://www.openlca.org/">openLCA&lt;/a>, and &lt;a href="https://www.ifu.com/umberto/">Umberto&lt;/a>. These are all proprietary software, and they&amp;rsquo;re expensive. But there&amp;rsquo;s a free and open source alternative: &lt;a href="https://docs.brightway.dev/en/latest/">Brightway&lt;/a>.&lt;/p></description></item><item><title>Fast Poetry and pre-commit with GitHub Actions</title><link>https://maxhalford.github.io/blog/fast-poetry-pre-commit-github-actions/</link><pubDate>Tue, 27 Feb 2024 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fast-poetry-pre-commit-github-actions/</guid><description>&lt;p>This is a short post to share a GitHub Actions pattern I use to setup &lt;a href="https://python-poetry.org/">Poetry&lt;/a> and &lt;a href="https://pre-commit.com/">pre-commit&lt;/a>. These two tools cover most of my Python development needs. I use Poetry to manage dependencies and pre-commit to run code checks and formatting. The setup is fast because it caches the virtual environment and the &lt;code>.local&lt;/code> directory.&lt;/p>
&lt;p>I like to use &lt;a href="https://docs.github.com/en/actions/creating-actions/about-custom-actions">custom actions&lt;/a> for this type of stuff. These are base actions that canÂ be re-used in multiple workflows. I have a custom action to install the Python environment. Here&amp;rsquo;s the action file:&lt;/p></description></item><item><title>Measuring the carbon footprint of pizzas</title><link>https://maxhalford.github.io/blog/carbon-footprint-pizzas/</link><pubDate>Sun, 25 Jun 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/carbon-footprint-pizzas/</guid><description>&lt;p>Making environmentally friendly decisions can only be done with the right information. At Carbonfact, we&amp;rsquo;ve realized a big challenge is the lack of information about industrial processes. We tackle that slowly but surely by gathering data from various sources, and making it available to our customers.&lt;/p>
&lt;p>Regarding food, the French government has a great initiative called &lt;a href="https://agribalyse.ademe.fr/">Agribalyse&lt;/a>. It&amp;rsquo;s a free database of environmental footprints for various food products. It includes raw ingredients straight out from the farm, as well as ready to eat dishes from the supermarket. It&amp;rsquo;s a great initiative, as it allows anyone to do their own research and make informed decisions.&lt;/p></description></item><item><title>Using SymPy in Python doctests</title><link>https://maxhalford.github.io/blog/sympy-doctests/</link><pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sympy-doctests/</guid><description>&lt;p>A program which compiles and runs without errors isn&amp;rsquo;t necessarily correct. I find this to be especially true for statistical software, both as a developer and as a user. Small but nasty bugs creep up on me every week. I keep sane in the membrane by writing many unit tests ðŸ›ðŸ”¨&lt;/p>
&lt;p>I make heavy use of &lt;a href="https://docs.python.org/3/library/doctest.html">doctests&lt;/a>. These are unit tests which you write as Python &lt;a href="https://realpython.com/documenting-python-code/#documenting-your-python-code-base-using-docstrings">docstrings&lt;/a>. They&amp;rsquo;re really handy because they kill two birds with one stone: the unit tests you write for a function also act as documentation.&lt;/p></description></item><item><title>Weighted sampling without replacement in pure Python</title><link>https://maxhalford.github.io/blog/weighted-sampling-without-replacement/</link><pubDate>Fri, 24 Dec 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/weighted-sampling-without-replacement/</guid><description>&lt;p>I&amp;rsquo;m working on a problem where I need to sample &lt;code>k&lt;/code> items from a list without replacement. The sampling has to be weighted. In Python, &lt;code>numpy&lt;/code> has &lt;code>random.choice&lt;/code> method which allows doing this:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">n&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">k&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">42&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">population&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">weights&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dirichlet&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones_like&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">population&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">choice&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">population&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">size&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">k&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">replace&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">p&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">weights&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">array&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">9&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>I&amp;rsquo;m always wary of using &lt;code>numpy&lt;/code> without thinking because I know it incurs some overhead. This overhead is usually meaningful when small amounts of data are involved. In such a case, a pure Python implementation may be faster.&lt;/p></description></item></channel></rss>