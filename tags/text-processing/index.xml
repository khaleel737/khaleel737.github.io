<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Text-Processing on Max Halford</title><link>https://maxhalford.github.io/tags/text-processing/</link><description>Recent content in Text-Processing on Max Halford</description><generator>Hugo</generator><language>en-us</language><managingEditor>maxhalford25@gmail.com (Max Halford)</managingEditor><webMaster>maxhalford25@gmail.com (Max Halford)</webMaster><lastBuildDate>Sun, 20 Nov 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://maxhalford.github.io/tags/text-processing/index.xml" rel="self" type="application/rss+xml"/><item><title>Parsing garment descriptions with GPT-3</title><link>https://maxhalford.github.io/blog/garment-parsing-gpt3/</link><pubDate>Sun, 20 Nov 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/garment-parsing-gpt3/</guid><description>&lt;h2 id="the-task">The task&lt;/h2>
&lt;p>You&amp;rsquo;ll have heard of GPT-3 if you haven&amp;rsquo;t been hiding under a rock. I&amp;rsquo;ve recently been impressed by Nat Friedman &lt;a href="https://twitter.com/natfriedman/status/1575631194032549888">teaching&lt;/a> GPT-3 to use a browser, and SeekWell &lt;a href="https://blog.seekwell.io/gpt3">generating&lt;/a> SQL queries from free-text. I think the most exciting usecases are yet to come. But GPT-3 has a good chance of changing the way we approach mundane tasks at work.&lt;/p>
&lt;p>I wrote an &lt;a href="https://maxhalford.github.io/blog/carbonfact-nlp-open-problem">article&lt;/a> a couple of months ago about a boring task I have to do at work. I got a few interesting suggestions by email. RaphaÃ«l suggested a &lt;a href="https://huggingface.co/tasks/question-answering">question-answering&lt;/a> model &lt;a href="https://raphaelsty.github.io/blog/qa/">here&lt;/a>. I&amp;rsquo;ve slowly become convinced that GPT-3 might be the perfect tool for the job. It&amp;rsquo;s cold and miserable where I am, so I thought it would be opportune to take GPT-3 for a spin ðŸ§™&lt;/p></description></item><item><title>NLP at Carbonfact: how would you do it?</title><link>https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/</link><pubDate>Tue, 06 Sep 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/carbonfact-nlp-open-problem/</guid><description>&lt;h2 id="the-task">The task&lt;/h2>
&lt;p>I work at a company called &lt;a href="https://www.carbonfact.com/">Carbonfact&lt;/a>. Our core value proposal is computing the &lt;a href="https://www.wikiwand.com/en/Carbon_footprint">carbon footprint&lt;/a> of clothing items, expressed in &lt;a href="https://www.wikiwand.com/en/Carbon_Dioxide_Equivalent">carbon dioxide equivalent&lt;/a> &amp;ndash; $kgCO_2e$ in short. For instance, we started by measuring the footprint of shoes &amp;ndash; no pun intended. We do these measurements with &lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment">life cycle analysis (LCA)&lt;/a> software we built ourselves. We use these analyses to fuel higher-level tasks for our clients, such as &lt;a href="https://www.wikiwand.com/en/Carbon_accounting">carbon accounting&lt;/a> and &lt;a href="https://www.wikiwand.com/en/Sustainable_procurement">sustainable procurement&lt;/a>.&lt;/p>
&lt;p>A life cycle analysis is essentially a recipe, the output of which is a carbon footprint assessment. Like any recipe, an LCA necessitates ingredients. In a &lt;a href="https://www.wikiwand.com/en/Life-cycle_assessment#/Cradle-to-gate">cradle-to-gate&lt;/a> scenario, this includes everything that is needed to make the product: the materials, the mass, the manufacturing methods, the transport between factories, etc. In our experience, the biggest impact on a product&amp;rsquo;s footprint come from the materials which it is made of.&lt;/p></description></item><item><title>Fuzzy regex matching in Python</title><link>https://maxhalford.github.io/blog/fuzzy-regex-matching-in-python/</link><pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/fuzzy-regex-matching-in-python/</guid><description>&lt;h2 id="fuzzy-string-matching-in-a-nutshell">Fuzzy string matching in a nutshell&lt;/h2>
&lt;p>Say we&amp;rsquo;re looking for a pattern in a blob of text. If you know the text has no typos, then determining whether it contains a pattern is trivial. In Python you can use the &lt;code>in&lt;/code> function. You can also write a regex pattern with the &lt;code>re&lt;/code> module from the standard library. But what about if the text contains typos? For instance, this might be the case with user inputs on a website, or with OCR outputs. This is a much harder problem.&lt;/p></description></item><item><title>OCR spelling correction is hard</title><link>https://maxhalford.github.io/blog/ocr-spelling-correction-is-hard/</link><pubDate>Sun, 06 Mar 2022 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/ocr-spelling-correction-is-hard/</guid><description>&lt;p>I recently saw &lt;a href="https://news.ycombinator.com/item?id=30576435">SymSpell&lt;/a> pop up on Hackernews. It claims to be a million times faster than &lt;a href="https://norvig.com/spell-correct.html">Peter Norvig&amp;rsquo;s spelling corrector&lt;/a>. I think it&amp;rsquo;s great that there&amp;rsquo;s a fast open source solution for spelling correction. But in my experience, the most challenging aspect of spelling correction is not necessarily speed.&lt;/p>
&lt;p>When I &lt;a href="https://maxhalford.github.io/blog/one-year-at-alan">worked at Alan&lt;/a>, I mostly wrote logic to extract structured information from medical documents. After some months working on the topic, I have to admit I hadn&amp;rsquo;t cracked the problem. The goal was to process &amp;gt;80% of documents with no human interaction, but when I left we had only reached 35%. However, I developed a good understanding of what made this task so difficult.&lt;/p></description></item><item><title>Homoglyphs: different characters that look identical</title><link>https://maxhalford.github.io/blog/homoglyphs/</link><pubDate>Thu, 19 Aug 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/homoglyphs/</guid><description>&lt;h2 id="a-wild-homoglyph-appears">A wild homoglyph appears&lt;/h2>
&lt;p>For instance, can you tell if there&amp;rsquo;s a difference between &lt;code>H&lt;/code> and &lt;code>Î—&lt;/code>? How about &lt;code>N&lt;/code> and &lt;code>Î&lt;/code>? These characters may seem identical, but they are actually different. You can try this out for yourself in Python:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="s1">&amp;#39;H&amp;#39;&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s1">&amp;#39;Î—&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kc">False&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="s1">&amp;#39;N&amp;#39;&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s1">&amp;#39;Î&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kc">False&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Indeed, these all represent different Unicode characters:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;H&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Î—&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="mi">72&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">919&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;N&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="nb">ord&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;Î&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">(&lt;/span>&lt;span class="mi">78&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">925&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>Î—&lt;/code> in fact represents the capital &lt;a href="https://www.wikiwand.com/en/Eta">Eta&lt;/a> letter, while &lt;code>Î&lt;/code> is a capital &lt;a href="https://www.wikiwand.com/en/Nu_(letter)">Nu&lt;/a>. In fact, entering &lt;code>H&lt;/code> or &lt;code>Î—&lt;/code> in Google will produce different results. The same goes for &lt;code>N&lt;/code> and &lt;code>Î&lt;/code>.&lt;/p></description></item><item><title>Automated document processing at Alan</title><link>https://maxhalford.github.io/blog/medium-document-processing/</link><pubDate>Thu, 10 Jun 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/medium-document-processing/</guid><description/></item><item><title>Text classification by data compression</title><link>https://maxhalford.github.io/blog/text-classification-by-compression/</link><pubDate>Tue, 08 Jun 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/text-classification-by-compression/</guid><description>&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>I posted this &lt;a href="https://news.ycombinator.com/item?id=27440093">on Hackernews&lt;/a> and got some valuable feedback. Many brought up the fact that you should be able to reuse the internal state of the compressor instead of recompressing the training data each time a prediction is made. There&amp;rsquo;s also some insightful references to data compression theory and its ties to statistical learning&lt;/em>&lt;/p>
&lt;p>Last night I felt like reading &lt;a href="http://aima.cs.berkeley.edu/">&lt;em>Artificial Intelligence: A Modern Approach&lt;/em>&lt;/a>. I stumbled on something fun in the natural language processing chapter. The section I was reading dealt with classifying text. The idea of the particular subsection I was reading was to classify documents by using a &lt;a href="https://www.wikiwand.com/en/Data_compression">compression algorithm&lt;/a>. This is such a left field idea, and yet it does make sense when you think about it. To quote the book:&lt;/p></description></item><item><title>Reducing the memory footprint of a scikit-learn text classifier</title><link>https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/</link><pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/sklearn-text-classifier-memory-footprint-reduction/</guid><description>&lt;h2 id="context">Context&lt;/h2>
&lt;p>This week at Alan I&amp;rsquo;ve been working on parsing &lt;a href="https://www.wikiwand.com/fr/Ordonnance_(m%C3%A9decine)">French medical prescriptions&lt;/a>. There are three types of prescriptions: lenses, glasses, and pharmaceutical prescriptions. Different information needs to be extracted depending on the prescription type. Therefore, the first step is to classify the prescription. The prescriptions we receive are pictures taken by users with their phone. We run each image through an OCR to obtain a text transcription of the image. We can thus use the text transcription to classify the prescription.&lt;/p></description></item><item><title>Converting Amazon Textract tables to pandas DataFrames</title><link>https://maxhalford.github.io/blog/textract-table-to-pandas/</link><pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/textract-table-to-pandas/</guid><description>&lt;p>I&amp;rsquo;m currently doing a lot of document processing at work. One of my tasks is to extract tables from PDF files. I evaluated &lt;a href="https://aws.amazon.com/textract/?nc1=h_ls">Amazon Textract&lt;/a>&amp;rsquo;s &lt;a href="https://docs.aws.amazon.com/textract/latest/dg/how-it-works-tables.html">table extraction&lt;/a> capability as part of this task. It&amp;rsquo;s very well documented, as is the rest of Textract. I was slightly disappointed by &lt;a href="https://docs.aws.amazon.com/textract/latest/dg/examples-blocks.html">the examples&lt;/a>, but nothing serious.&lt;/p>
&lt;p>I wanted to write this short blog post to share a piece of code I use to convert tables extracted through Amazon Textract to &lt;a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html">&lt;code>pandas.DataFrame&lt;/code>&lt;/a>s. I&amp;rsquo;ll be using the following anonymised image as an example:&lt;/p></description></item><item><title>Unsupervised text classification with word embeddings</title><link>https://maxhalford.github.io/blog/unsupervised-text-classification/</link><pubDate>Sat, 03 Oct 2020 00:00:00 +0000</pubDate><author>maxhalford25@gmail.com (Max Halford)</author><guid>https://maxhalford.github.io/blog/unsupervised-text-classification/</guid><description>&lt;div align="center" >
 &lt;img height="300px" src="https://maxhalford.github.io/img/blog/document-classification/morpheus.jpg" alt="morpheus">
 &lt;br>
&lt;/div>
&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>since writing this article, I have discovered that the method I describe is a form of &lt;a href="https://www.wikiwand.com/en/Zero-shot_learning">zero-shot learning&lt;/a>. So I guess you could say that this article is a tutorial on zero-shot learning for NLP.&lt;/em>&lt;/p>
&lt;p>&lt;strong>Edit&lt;/strong> &amp;ndash; &lt;em>I stumbled on a &lt;a href="https://www.aclweb.org/anthology/P19-1036/">paper&lt;/a> entitled &amp;ldquo;Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings&amp;rdquo; which proposes something very similar. The paper is rather well written, so you might want to check it out. Note that they call the &lt;code>tech -&amp;gt; technology&lt;/code> trick &amp;ldquo;label enrichment&amp;rdquo;.&lt;/em>&lt;/p></description></item></channel></rss>